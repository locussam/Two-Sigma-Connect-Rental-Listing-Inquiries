{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/resources/common/.virtualenv/python3/lib/python3.4/site-packages/sklearn/cross_validation.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n",
      "/usr/local/lib/python3.4/dist-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import wget\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import random\n",
    "import string\n",
    "from scipy import sparse\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#wget.download(\"https://kaggle2.blob.core.windows.net/competitions-data/kaggle/5590/train.json.zip?sv=2015-12-11&sr=b&sig=VJrcQLsw1uvlRYlQ7eh03u2Y%2B94O%2FSpMM1SkMiwRBsg%3D&se=2017-04-17T08%3A14%3A41Z&sp=r\")\n",
    "#wget.download(\"https://kaggle2.blob.core.windows.net/competitions-data/kaggle/5590/test.json.zip?sv=2015-12-11&sr=b&sig=a%2BoFo011fFElFejTowliMvSBOIPdDuyumfxOzij1uW0%3D&se=2017-04-17T08%3A14%3A44Z&sp=r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!unzip train.json.zip\n",
    "#!unzip test.json.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few features I created but didn't used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"train.json\")\n",
    "test_df = pd.read_json(\"test.json\")\n",
    "\n",
    "\n",
    "y = train_df['interest_level']\n",
    "train_df.drop('interest_level',axis = 1, inplace = True)\n",
    "train = train_df\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df, y_train, y_test =train_test_split( train, y, test_size=0.33, random_state=42)\n",
    "\n",
    "train_df['interest_level'] = y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "#Observed from https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths/notebook\n",
    "train_df['zero_b'] = 0\n",
    "test_df['zero_b'] = 0\n",
    "train_df.loc[train_df.bathrooms==0,\"zero_b\"] = 1\n",
    "test_df.loc[test_df.bathrooms==0,\"zero_b\"] = 1\n",
    "\n",
    "train_df['p_five_b'] = 0\n",
    "test_df['p_five_b'] = 0\n",
    "train_df.loc[train_df.bathrooms==0.5,\"p_five_b\"] = 1\n",
    "test_df.loc[test_df.bathrooms==0.5,\"p_five_b\"] = 1\n",
    "\n",
    "\n",
    "train_df['one_b'] = 0\n",
    "test_df['one_b'] = 0\n",
    "train_df.loc[train_df.bathrooms==1,\"one_b\"] = 1\n",
    "test_df.loc[test_df.bathrooms==1,\"one_b\"] = 1\n",
    "\n",
    "train_df['p_one_b'] = 0\n",
    "test_df['p_one_b'] = 0\n",
    "train_df.loc[train_df.bathrooms==1.5,\"p_one_b\"] = 1\n",
    "test_df.loc[test_df.bathrooms==1.5,\"p_one_b\"] = 1\n",
    "\n",
    "\n",
    "train_df['two_b'] = 0\n",
    "test_df['two_b'] = 0\n",
    "train_df.loc[train_df.bathrooms==2,\"two_b\"] = 1\n",
    "test_df.loc[test_df.bathrooms==2,\"two_b\"] = 1\n",
    "\n",
    "train_df['p_two_b'] = 0\n",
    "test_df['p_two_b'] = 0\n",
    "train_df.loc[train_df.bathrooms==2.5,\"p_two_b\"] = 1\n",
    "test_df.loc[test_df.bathrooms==2.5,\"p_two_b\"] = 1\n",
    "\n",
    "train_df['three_b'] = 0\n",
    "test_df['three_b'] = 0\n",
    "train_df.loc[train_df.bathrooms==3,\"three_b\"] = 1\n",
    "test_df.loc[test_df.bathrooms==3,\"three_b\"] = 1\n",
    "\n",
    "train_df['four_b'] = 0\n",
    "test_df['four_b'] = 0\n",
    "train_df.loc[train_df.bathrooms==4,\"four_b\"] = 1\n",
    "test_df.loc[test_df.bathrooms==4,\"four_b\"] = 1\n",
    "\n",
    "\n",
    "train_df[\"per_sqft\"] = train_df[\"price\"]/(1+train_df[\"bedrooms\"].clip(1,4)+train_df[\"bathrooms\"].clip(0,2))\n",
    "test_df[\"per_sqft\"] = test_df[\"price\"]/(1+test_df[\"bedrooms\"].clip(1,4)+test_df[\"bathrooms\"].clip(0,2))\n",
    "\n",
    "#https://www.kaggle.com/zeroblue/two-sigma-connect-rental-listing-inquiries/simple-starter-keras-nn/code\n",
    "#is_digit = lambda x: str(x).isdigit()\n",
    "\n",
    "\n",
    " \n",
    "\n",
    "#train_df['addr_has_number'] = train_df['display_address'].str.split().str.get(0)\n",
    "#train_df['addr_has_number'] = train_df['addr_has_number'].apply(is_digit)\n",
    "\n",
    "#test_df['addr_has_number'] = test_df['display_address'].str.split().str.get(0)\n",
    "#test_df['addr_has_number'] = test_df['addr_has_number'].apply(is_digit)\n",
    "\n",
    "\n",
    "train_df['half_bathrooms'] = train_df[\"bathrooms\"] - train_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "test_df['half_bathrooms'] = test_df[\"bathrooms\"] - test_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "\n",
    "train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "\n",
    "train_df[\"price_b\"] =train_df[\"price\"]/train_df[\"bathrooms\"]\n",
    "test_df[\"price_b\"] = test_df[\"price\"]/test_df[\"bathrooms\"] \n",
    "\n",
    "train_df[\"price_hb\"] =train_df[\"price\"]/train_df[\"half_bathrooms\"]\n",
    "test_df[\"price_hb\"] = test_df[\"price\"]/test_df[\"half_bathrooms\"] \n",
    "\n",
    "\n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "\n",
    "train_df['price_per_room'] = train_df['price']/train_df['room_sum']\n",
    "test_df['price_per_room'] = test_df['price']/test_df['room_sum']\n",
    "\n",
    "train_df[\"bedBathDiffP\"] = train_df[\"price_t\"] - train_df[\"half_bathrooms\"]\n",
    "test_df[\"bedBathDiffP\"] = test_df[\"price_t\"] - test_df[\"half_bathrooms\"]\n",
    "\n",
    "train_df[\"bedBathSumP\"] = train_df[\"price_t\"] + train_df[\"half_bathrooms\"]\n",
    "test_df[\"bedBathSumP\"] = test_df[\"price_t\"] + test_df[\"half_bathrooms\"]\n",
    "\n",
    "train_df[\"bedBathDiff\"] = train_df[\"bedrooms\"] - train_df[\"bathrooms\"]\n",
    "test_df[\"bedBathDiff\"] = test_df[\"bedrooms\"] - test_df[\"bathrooms\"]\n",
    "\n",
    "train_df[\"bedBathSum\"] = train_df[\"bedrooms\"] + train_df[\"bathrooms\"]\n",
    "test_df[\"bedBathSum\"] = test_df[\"bedrooms\"] + test_df[\"bathrooms\"]\n",
    "\n",
    "\n",
    "train_df[\"bedsPerc\"] = train_df[\"bedrooms\"] / (train_df['bedrooms'] + train_df['bathrooms'])\n",
    "test_df[\"bedsPerc\"] = test_df[\"bedrooms\"] / (test_df['bedrooms'] + test_df['bathrooms'])\n",
    "\n",
    "\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "train_df[\"created_weekday\"] = train_df[\"created\"].dt.weekday\n",
    "test_df[\"created_weekday\"] = test_df[\"created\"].dt.weekday\n",
    "train_df[\"created_week\"] = train_df[\"created\"].dt.week\n",
    "test_df[\"created_week\"] = test_df[\"created\"].dt.week\n",
    "\n",
    "train_df['num_description_words'] = train_df['description'].apply(lambda x: len(x.split(' ')))\n",
    "train_df['num_description_len'] = train_df['description'].apply(len)\n",
    "\n",
    "test_df['num_description_words'] = test_df['description'].apply(lambda x: len(x.split(' ')))\n",
    "test_df['num_description_len'] = test_df['description'].apply(len)\n",
    "\n",
    "\n",
    "train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "train_df['bad_building'] = 0\n",
    "train_df.loc[train_df.building_id == 0,'bad_building'] = 1\n",
    "\n",
    "test_df['bad_building'] = 0\n",
    "test_df.loc[test_df.building_id == 0,'bad_building'] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Features from https://www.kaggle.com/rakhlin/two-sigma-connect-rental-listing-inquiries/another-python-version-of-it-is-lit-by-branden\n",
    "\n",
    "def factorize(df1, df2, column):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    factors = ps.factorize()[0]\n",
    "    df1[column] = factors[:len(df1)]\n",
    "    df2[column] = factors[len(df1):]\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "def designate_single_observations(df1, df2, column):\n",
    "    ps = df1[column].append(df2[column])\n",
    "    grouped = ps.groupby(ps).size().to_frame().rename(columns={0: \"size\"})\n",
    "    df1.loc[df1.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n",
    "    df2.loc[df2.join(grouped, on=column, how=\"left\")[\"size\"] <= 1, column] = -1\n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "def hcc_encode(train_df, test_df, variable, target, prior_prob, k, f=1, g=1, r_k=None, update_df=None):\n",
    "    \"\"\"\n",
    "    See \"A Preprocessing Scheme for High-Cardinality Categorical Attributes in\n",
    "    Classification and Prediction Problems\" by Daniele Micci-Barreca\n",
    "    \"\"\"\n",
    "    hcc_name = \"_\".join([\"hcc\", variable, target])\n",
    "\n",
    "    grouped = train_df.groupby(variable)[target].agg({\"size\": \"size\", \"mean\": \"mean\"})\n",
    "    grouped[\"lambda\"] = 1 / (g + np.exp((k - grouped[\"size\"]) / f))\n",
    "    grouped[hcc_name] = grouped[\"lambda\"] * grouped[\"mean\"] + (1 - grouped[\"lambda\"]) * prior_prob\n",
    "\n",
    "    df = test_df[[variable]].join(grouped, on=variable, how=\"left\")[hcc_name].fillna(prior_prob)\n",
    "    if r_k: df *= np.random.uniform(1 - r_k, 1 + r_k, len(test_df))     # Add uniform noise. Not mentioned in original paper\n",
    "\n",
    "    if update_df is None: update_df = test_df\n",
    "    if hcc_name not in update_df.columns: update_df[hcc_name] = np.nan\n",
    "    update_df.update(df)\n",
    "    return\n",
    "\n",
    "\n",
    "def create_binary_features(df):\n",
    "    bows = {\n",
    "        \"dogs\": (\"dogs\", \"dog\"),\n",
    "        \"cats\": (\"cats\",),\n",
    "        \"nofee\": (\"no fee\", \"no-fee\", \"no  fee\", \"nofee\", \"no_fee\"),\n",
    "        \"lowfee\": (\"reduced_fee\", \"low_fee\", \"reduced fee\", \"low fee\"),\n",
    "        \"furnished\": (\"furnished\",),\n",
    "        \"parquet\": (\"parquet\", \"hardwood\"),\n",
    "        \"concierge\": (\"concierge\", \"doorman\", \"housekeep\", \"in_super\"),\n",
    "        \"prewar\": (\"prewar\", \"pre_war\", \"pre war\", \"pre-war\"),\n",
    "        \"laundry\": (\"laundry\", \"lndry\"),\n",
    "        \"health\": (\"health\", \"gym\", \"fitness\", \"training\"),\n",
    "        \"transport\": (\"train\", \"subway\", \"transport\"),\n",
    "        \"parking\": (\"parking\",),\n",
    "        \"utilities\": (\"utilities\", \"heat water\", \"water included\")\n",
    "    }\n",
    "\n",
    "    def indicator(bow):\n",
    "        return lambda s: int(any([x in s for x in bow]))\n",
    "\n",
    "    features = df[\"features\"].apply(lambda f: \" \".join(f).lower())   # convert features to string\n",
    "    for key in bows:\n",
    "        df[\"feature_\" + key] = features.apply(indicator(bows[key]))\n",
    "\n",
    "    return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(train_df['half_bathrooms'].describe())\n",
    "x = pd.crosstab(train_df.bathrooms, train_df.interest_level)[['low', 'medium', 'high']]\n",
    "x.div(x.sum(1), 0).plot(kind='bar', color=['red', 'yellow', 'green'], stacked=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from itertools import product\n",
    "\n",
    "#train_df = pd.read_json(\"train.json\").sort_values(by=\"listing_id\")\n",
    "#test_df = pd.read_json(\"test.json\").sort_values(by=\"listing_id\")\n",
    "\n",
    "train_df['int_lvl'] = train_df['interest_level']\n",
    "# Make target integer, one hot encoded, calculate target priors\n",
    "train_df = train_df.replace({\"interest_level\": {\"low\": 0, \"medium\": 1, \"high\": 2}})\n",
    "train_df = train_df.join(pd.get_dummies(train_df[\"interest_level\"], prefix=\"pred\").astype(int))\n",
    "prior_0, prior_1, prior_2 = train_df[[\"pred_0\", \"pred_1\", \"pred_2\"]].mean()\n",
    "\n",
    "# Special designation for building_ids, manager_ids, display_address with only 1 observation\n",
    "for col in ('building_id', 'manager_id', 'display_address'):\n",
    "    train_df, test_df = designate_single_observations(train_df, test_df, col)\n",
    "\n",
    "# High-Cardinality Categorical encoding\n",
    "skf = StratifiedKFold(10)\n",
    "attributes = product((\"building_id\", \"manager_id\"), zip((\"pred_1\", \"pred_2\"), (prior_1, prior_2)))\n",
    "for variable, (target, prior) in attributes:\n",
    "    hcc_encode(train_df, test_df, variable, target, prior, k=5, r_k=None)\n",
    "    for train, test in skf.split(np.zeros(len(train_df)), train_df['interest_level']):\n",
    "        hcc_encode(train_df.iloc[train], train_df.iloc[test], variable, target, prior, k=5, r_k=0.01, update_df=train_df)\n",
    "\n",
    "# Factorize building_id, display_address, manager_id, street_address\n",
    "for col in ('building_id', 'display_address', 'manager_id', 'street_address'):\n",
    "    train_df, test_df = factorize(train_df, test_df, col)\n",
    "\n",
    "# Create binarized features\n",
    "train_df = create_binary_features(train_df)\n",
    "test_df = create_binary_features(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['interest_level'] = train_df['int_lvl']\n",
    "\n",
    "train_df.drop(['int_lvl', 'pred_0', 'pred_1', 'pred_2'],axis = 1, inplace=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(train_df.columns)\n",
    "print(train_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing outliers from latitude and longitude\n",
    "for i in ['latitude', 'longitude']:\n",
    "    while(1):\n",
    "        x = train_df[i].median()\n",
    "        ix = abs(train_df[i] - x) > 3*train_df[i].std()\n",
    "        if ix.sum()==0: # no more outliers -> stop\n",
    "            break\n",
    "        train_df.loc[ix, i] = np.nan # exclude outliers\n",
    "# Keep only non-outlier listings\n",
    "train_df = train_df.loc[train_df[['latitude', 'longitude']].isnull().sum(1) == 0, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in ['latitude', 'longitude']:\n",
    "    while(1):\n",
    "        x = test_df[i].median()\n",
    "        ix = abs(test_df[i] - x) > 3*test_df[i].std()\n",
    "        if ix.sum()==0: # no more outliers -> stop\n",
    "            break\n",
    "        test_df.loc[ix, i] = np.nan # exclude outliers\n",
    "# Keep only non-outlier listings\n",
    "#test_df.loc[test_df[['latitude', 'longitude']].isnull().sum(1) == 0, :]\n",
    "test_df['longitude'].fillna(test_df['longitude'].mean(),inplace=True)\n",
    "test_df['latitude'].fillna(test_df['latitude'].mean(),inplace=True)\n",
    "test_df[['latitude', 'longitude']].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#K- Means and DBSCAN Clustring and storking cluster ID as a features\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "\n",
    "for ix, ncomp in enumerate([10]):\n",
    "    r = KMeans(ncomp, random_state=1)\n",
    "    # Normalize (longitude, latitude) before K-means\n",
    "    temp = pd.concat((train_df[['longitude', 'latitude']], test_df[['longitude', 'latitude']]), axis=0).reset_index(drop=True).copy()\n",
    "    temp['longitude'] = (temp['longitude']-temp['longitude'].mean())/temp['longitude'].std()\n",
    "    temp['latitude'] = (temp['latitude']-temp['latitude'].mean())/temp['latitude'].std()\n",
    "    # Fit k-means and get labels\n",
    "    r.fit(temp[['longitude', 'latitude']])\n",
    "    a = r.labels_\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "for ix, ncomp in enumerate([10]):\n",
    "    r = DBSCAN(eps=2,min_samples=5, metric=\"precomputed\")\n",
    "    # Normalize (longitude, latitude) before K-means\n",
    "    temp = pd.concat((train_df[['longitude', 'latitude']], test_df[['longitude', 'latitude']]), axis=0).reset_index(drop=True).copy()\n",
    "    temp['longitude'] = (temp['longitude']-temp['longitude'].mean())/temp['longitude'].std()\n",
    "    temp['latitude'] = (temp['latitude']-temp['latitude'].mean())/temp['latitude'].std()\n",
    "    # Fit k-means and get labels\n",
    "    r.fit_predict(temp[['longitude', 'latitude']])\n",
    "    a = r.labels_\n",
    "\n",
    "\n",
    "ntrain = train_df.shape[0]     \n",
    "train_df['cluster'] = a[:ntrain]\n",
    "test_df['cluster'] = a[ntrain:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/zeroblue/two-sigma-connect-rental-listing-inquiries/simple-starter-keras-nn/code\n",
    "#Create ratios\n",
    "test_df['interest_level'] = \"High\"\n",
    "\n",
    "ntrain = train_df.shape[0]\n",
    "all_data = train_df.append(test_df)\n",
    "\n",
    "\n",
    "median_list = ['bedrooms', 'bathrooms', 'building_id', 'manager_id']\n",
    "#, 'cluster']\n",
    "for col in median_list:\n",
    "    median_price = all_data[[col, 'price']].groupby(col)['price'].median()\n",
    "    median_price = median_price[all_data[col]].values.astype(float)\n",
    "    all_data['median_' + col] = median_price\n",
    "    all_data['ratio_' + col] = all_data['price'] / median_price\n",
    "    all_data['median_' + col] = np.log(all_data['median_' + col].values)\n",
    "\n",
    "\n",
    "test_df.drop('interest_level',axis=1,inplace=True)    \n",
    "train_df = all_data[:ntrain]\n",
    "test_df = all_data[ntrain:]\n",
    "\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#KNN for Lat and Long\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "classifier = KNeighborsClassifier(n_neighbors = 5) \n",
    "\n",
    "#Fit model with training data\n",
    "classifier.fit(train_df[['latitude','longitude']], train_df['interest_level'])\n",
    "\n",
    "#Predict Output\n",
    "predicted_test= classifier.predict(test_df[['latitude','longitude']])\n",
    "\n",
    "predicted_train= classifier.predict(train_df[['latitude','longitude']])\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "knn_score = accuracy_score(train_df['interest_level'],predicted_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['knn_outcome'] = predicted_train\n",
    "test_df['knn_outcome'] = predicted_test\n",
    "\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_df['knn_outcome'] = np.array(train_df['knn_outcome'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "test_df['knn_outcome'] = np.array(test_df['knn_outcome'].apply(lambda x: target_num_map[x]))\n",
    "\n",
    "print(plt.hist(test_df['knn_outcome']))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "categorical = [\"street_address\", \"display_address\", \"manager_id\", \"building_id\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_._\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_._\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(train_df[features_to_use].dtypes == test_df[features_to_use].dtypes)\n",
    "test_df[features_to_use].isnull().sum()\n",
    "#tr_sparse.dtype\n",
    "#train_df[features_to_use].fillna(train_df[features_to_use].mean(),inplace = True)\n",
    "#test_df[features_to_use].fillna(test_df[features_to_use].mean(),inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#train_df[features_to_use].fillna(train_df[features_to_use].mean(),inplace = True)\n",
    "#test_df[features_to_use].fillna(test_df[features_to_use].mean(),inplace = True)\n",
    "\n",
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "test_y = np.array(y_test.apply(lambda x: target_num_map[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_scores = []\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_Y, val_Y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_Y, val_X, val_Y)\n",
    "        cv_scores.append(log_loss(val_Y, preds))\n",
    "        print(cv_scores)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.53256323037364017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My Train-Test Split for CV | Go an Train the model now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def multiclass_log_loss(y_true, y_pred, eps=1e-15):\n",
    "    \"\"\"Multi class version of Logarithmic Loss metric.\n",
    "    https://www.kaggle.com/wiki/MultiClassLogLoss\n",
    "\n",
    "    idea from this post:\n",
    "    http://www.kaggle.com/c/emc-data-science/forums/t/2149/is-anyone-noticing-difference-betwen-validation-and-leaderboard-error/12209#post12209\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples]\n",
    "    y_pred : array, shape = [n_samples, n_classes]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "    \"\"\"\n",
    "    predictions = np.clip(y_pred, eps, 1 - eps)\n",
    "\n",
    "    # normalize row sums to 1\n",
    "    predictions /= predictions.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    actual = np.zeros(y_pred.shape)\n",
    "    rows = actual.shape[0]\n",
    "    actual[np.arange(rows), y_true.astype(int)] = 1\n",
    "    vsota = np.sum(actual * np.log(predictions))\n",
    "    return -1.0 / rows * vsota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_X = train_df[features_to_use]\n",
    "test_X = test_df[features_to_use]\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random.seed = 0\n",
    "\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "sss = StratifiedShuffleSplit(train_y, 3, test_size=0.2, random_state=0)\n",
    "\n",
    "for dev_index, val_index in sss:\n",
    "    dev_X, val_X = train_X.iloc[dev_index], train_X.iloc[val_index]\n",
    "    dev_Y, val_Y = train_y[dev_index], train_y[val_index]\n",
    "print(dev_X.shape)\n",
    "print(val_X.shape)\n",
    "print(val_X.columns not in dev_X.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds, model = runXGB(dev_X, dev_Y, val_X,num_rounds=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "multiclass_log_loss(test_y,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.5318"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 40\n",
    "\n",
    "xgb.plot_importance(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_v2.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('xgb_v2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/chengzhan/two-sigma-connect-rental-listing-inquiries/xgb0415/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_df = pd.read_json(\"train.json\")\n",
    "test_df = pd.read_json(\"test.json\")\n",
    "\n",
    "\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=321, num_rounds=3000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.021\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['nthread'] = 16 \n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20, verbose_eval = 100)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "test_df[\"bathrooms\"].loc[19671] = 1.5\n",
    "test_df[\"bathrooms\"].loc[22977] = 2.0\n",
    "test_df[\"bathrooms\"].loc[63719] = 2.0\n",
    "train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "\n",
    "image_date = pd.read_csv(\"listing_image_time0.csv\")\n",
    "\n",
    "    # rename columns so you can join tables later on\n",
    "image_date.columns = [\"listing_id\", \"time_stamp\"]\n",
    "\n",
    "    # reassign the only one timestamp from April, all others from Oct/Nov\n",
    "image_date.loc[80240,\"time_stamp\"] = 1478129766 \n",
    "\n",
    "image_date[\"img_date\"]                  = pd.to_datetime(image_date[\"time_stamp\"], unit=\"s\")\n",
    "image_date[\"img_days_passed\"]           = (image_date[\"img_date\"].max() - image_date[\"img_date\"]).astype(\"timedelta64[D]\").astype(int)\n",
    "image_date[\"img_date_month\"]            = image_date[\"img_date\"].dt.month\n",
    "image_date[\"img_date_week\"]             = image_date[\"img_date\"].dt.week\n",
    "image_date[\"img_date_day\"]              = image_date[\"img_date\"].dt.day\n",
    "image_date[\"img_date_dayofweek\"]        = image_date[\"img_date\"].dt.dayofweek\n",
    "image_date[\"img_date_dayofyear\"]        = image_date[\"img_date\"].dt.dayofyear\n",
    "image_date[\"img_date_hour\"]             = image_date[\"img_date\"].dt.hour\n",
    "image_date[\"img_date_monthBeginMidEnd\"] = image_date[\"img_date_day\"].apply(lambda x: 1 if x<10 else 2 if x<20 else 3)\n",
    "\n",
    "train_df = pd.merge(train_df, image_date, on=\"listing_id\", how=\"left\")\n",
    "test_df = pd.merge(test_df, image_date, on=\"listing_id\", how=\"left\")\n",
    "    \n",
    "test_df[\"bathrooms\"].loc[19671] = 1.5\n",
    "test_df[\"bathrooms\"].loc[22977] = 2.0\n",
    "test_df[\"bathrooms\"].loc[63719] = 2.0\n",
    "train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "train_df['exl_count'] = train_df[\"description\"].str.count(\"!\")\n",
    "test_df['exl_count'] = test_df[\"description\"].str.count(\"!\")\n",
    "\n",
    "train_df[\"per_sqft\"] = train_df[\"price\"]/(1+train_df[\"bedrooms\"].clip(1,4)+train_df[\"bathrooms\"].clip(0,2))\n",
    "test_df[\"per_sqft\"] = test_df[\"price\"]/(1+test_df[\"bedrooms\"].clip(1,4)+test_df[\"bathrooms\"].clip(0,2))\n",
    "\n",
    "\n",
    "train_df['half_bathrooms'] = train_df[\"bathrooms\"] - train_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "test_df['half_bathrooms'] = test_df[\"bathrooms\"] - test_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "\n",
    "train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "\n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "\n",
    "train_df['price_per_room'] = train_df['price']/train_df['room_sum']\n",
    "test_df['price_per_room'] = test_df['price']/test_df['room_sum']\n",
    "\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "train_df[\"created_weekday\"] = train_df[\"created\"].dt.weekday\n",
    "test_df[\"created_weekday\"] = test_df[\"created\"].dt.weekday\n",
    "train_df[\"created_week\"] = train_df[\"created\"].dt.week\n",
    "test_df[\"created_week\"] = test_df[\"created\"].dt.week\n",
    "\n",
    "train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "features_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\"price_t\",\"price_per_room\", \"logprice\", \"density\", \"half_bathrooms\",\n",
    "\"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\", \"created_year\", \"created_month\", \"created_day\", \"created_hour\", \"created_week\", \"created_weekday\", \"exl_count\", \"per_sqft\"]\n",
    "\n",
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    \n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    \n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "            \n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "            \n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')\n",
    "\n",
    "categorical = [\"street_address\", \"display_address\", \"manager_id\", \"building_id\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)\n",
    "\n",
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])\n",
    "\n",
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "\n",
    "cv_scores = []\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_Y, val_Y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_Y, val_X, val_Y)\n",
    "        cv_scores.append(log_loss(val_Y, preds))\n",
    "        print(cv_scores)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(5/4)*1239"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#0.53436758843932697\n",
    "#With price per sq feet. 0.52887449500894068\n",
    "#Stopping. Best iteration:\n",
    "#[1477]\ttrain-mlogloss:0.35289\ttest-mlogloss:0.52884\n",
    "\n",
    "#[0.52887449500894068]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=1750)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "out_df.to_csv(\"xgb_new.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('xgb_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_date = pd.read_csv(\"listing_image_time0.csv\")\n",
    "\n",
    "# rename columns so you can join tables later on\n",
    "image_date.columns = [\"listing_id\", \"time_stamp\"]\n",
    "\n",
    "# reassign the only one timestamp from April, all others from Oct/Nov\n",
    "image_date.loc[80240,\"time_stamp\"] = 1478129766 \n",
    "\n",
    "image_date[\"img_date\"]                  = pd.to_datetime(image_date[\"time_stamp\"], unit=\"s\")\n",
    "image_date[\"img_days_passed\"]           = (image_date[\"img_date\"].max() - image_date[\"img_date\"]).astype(\"timedelta64[D]\").astype(int)\n",
    "image_date[\"img_date_month\"]            = image_date[\"img_date\"].dt.month\n",
    "image_date[\"img_date_week\"]             = image_date[\"img_date\"].dt.week\n",
    "image_date[\"img_date_day\"]              = image_date[\"img_date\"].dt.day\n",
    "image_date[\"img_date_dayofweek\"]        = image_date[\"img_date\"].dt.dayofweek\n",
    "image_date[\"img_date_dayofyear\"]        = image_date[\"img_date\"].dt.dayofyear\n",
    "image_date[\"img_date_hour\"]             = image_date[\"img_date\"].dt.hour\n",
    "image_date[\"img_date_monthBeginMidEnd\"] = image_date[\"img_date_day\"].apply(lambda x: 1 if x<10 else 2 if x<20 else 3)\n",
    "\n",
    "image_date.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Mon Mar 20 12:47:04 2017\n",
    "\n",
    "@author: Michael Hartman\n",
    "\n",
    "This is a simple Keras NN\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "from datetime import timedelta\n",
    "#from sklearn.model_selection import StratifiedKFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras import optimizers\n",
    "#from keras.callbacks import EarlyStopping\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "\n",
    "label_column = 'interest_level'\n",
    "num_classes = 3\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "train_file = \"train.json\"\n",
    "test_file = \"test.json\"\n",
    "train = pd.read_json(train_file)\n",
    "test = pd.read_json(test_file)\n",
    "\n",
    "    \n",
    "image_date = pd.read_csv(\"listing_image_time0.csv\")\n",
    "\n",
    "# rename columns so you can join tables later on\n",
    "image_date.columns = [\"listing_id\", \"time_stamp\"]\n",
    "\n",
    "# reassign the only one timestamp from April, all others from Oct/Nov\n",
    "image_date.loc[80240,\"time_stamp\"] = 1478129766 \n",
    "\n",
    "image_date[\"img_date\"]                  = pd.to_datetime(image_date[\"time_stamp\"], unit=\"s\")\n",
    "image_date[\"img_days_passed\"]           = (image_date[\"img_date\"].max() - image_date[\"img_date\"]).astype(\"timedelta64[D]\").astype(int)\n",
    "image_date[\"img_date_month\"]            = image_date[\"img_date\"].dt.month\n",
    "image_date[\"img_date_week\"]             = image_date[\"img_date\"].dt.week\n",
    "image_date[\"img_date_day\"]              = image_date[\"img_date\"].dt.day\n",
    "image_date[\"img_date_dayofweek\"]        = image_date[\"img_date\"].dt.dayofweek\n",
    "image_date[\"img_date_dayofyear\"]        = image_date[\"img_date\"].dt.dayofyear\n",
    "image_date[\"img_date_hour\"]             = image_date[\"img_date\"].dt.hour\n",
    "image_date[\"img_date_monthBeginMidEnd\"] = image_date[\"img_date_day\"].apply(lambda x: 1 if x<10 else 2 if x<20 else 3)\n",
    "\n",
    "train = pd.merge(train, image_date, on=\"listing_id\", how=\"left\")\n",
    "test = pd.merge(test, image_date, on=\"listing_id\", how=\"left\")\n",
    "\n",
    "\n",
    "# Make the label numeric\n",
    "label_map = pd.Series({'low': 2, 'medium': 1, 'high': 0})\n",
    "train[label_column] = label_map[train[label_column]].values\n",
    "\n",
    "all_data = train.append(test)\n",
    "all_data.set_index('listing_id', inplace=True)\n",
    "\n",
    "print('Identify bad geographic coordinates')\n",
    "all_data['bad_addr'] = 0\n",
    "mask = ~all_data['latitude'].between(40.5, 40.9)\n",
    "mask = mask | ~all_data['longitude'].between(-74.05, -73.7)\n",
    "bad_rows = all_data[mask]\n",
    "all_data.loc[mask, 'bad_addr'] = 1\n",
    "\n",
    "print('Create neighborhoods')\n",
    "# Replace bad values with mean\n",
    "mean_lat = all_data.loc[all_data['bad_addr']==0, 'latitude'].mean()\n",
    "all_data.loc[all_data['bad_addr']==1, 'latitude'] = mean_lat\n",
    "mean_long = all_data.loc[all_data['bad_addr']==0, 'longitude'].mean()\n",
    "all_data.loc[all_data['bad_addr']==1, 'longitude'] = mean_long\n",
    "# From: https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/unsupervised-and-supervised-neighborhood-encoding\n",
    "kmean_model = KMeans(42)\n",
    "loc_df = all_data[['longitude', 'latitude']].copy()\n",
    "standardize = lambda x: (x - x.mean()) / x.std()\n",
    "loc_df['longitude'] = standardize(loc_df['longitude'])\n",
    "loc_df['latitude'] = standardize(loc_df['latitude'])\n",
    "kmean_model.fit(loc_df)\n",
    "all_data['neighborhoods'] = kmean_model.labels_\n",
    "\n",
    "#print('Distance from center')\n",
    "lat = np.square(all_data['latitude'] - mean_lat)\n",
    "lng = np.square(all_data['longitude'] - mean_long)\n",
    "all_data['dist_from_center'] = np.sqrt(lat + lng)\n",
    "\n",
    "print('Fix Bathrooms')\n",
    "mask = all_data['bathrooms'] > 9\n",
    "all_data.loc[mask, 'bathrooms'] = 1\n",
    "\n",
    "print('Break up the date data')\n",
    "all_data['created'] = pd.to_datetime(all_data['created'])\n",
    "#all_data['year'] = all_data['created'].dt.year\n",
    "all_data['month'] = all_data['created'].dt.month\n",
    "all_data['day_of_month'] = all_data['created'].dt.day\n",
    "all_data['weekday'] = all_data['created'].dt.dayofweek\n",
    "all_data['day_of_year'] = all_data['created'].dt.dayofyear\n",
    "all_data['hour'] = all_data['created'].dt.hour\n",
    "\n",
    "all_data['count_feat'] = all_data['features'].apply(len)\n",
    "all_data['count_desc'] = all_data['description'].str.split().apply(len)\n",
    "\n",
    "all_data['addr_has_number'] = all_data['display_address'].str.split().str.get(0)\n",
    "is_digit = lambda x: str(x).isdigit()\n",
    "all_data['addr_has_number'] = all_data['addr_has_number'].apply(is_digit)\n",
    "\n",
    "print('Bed and bath features')\n",
    "all_data['bedrooms'] += 1\n",
    "all_data['bed_to_bath'] = all_data['bathrooms'] \n",
    "all_data['bed_to_bath'] /= all_data['bedrooms']\n",
    "all_data['price_per_bed'] = all_data['price'] / all_data['bedrooms']\n",
    "bath = all_data['bathrooms'].copy()\n",
    "bath.loc[all_data['bathrooms']==0] = 1\n",
    "all_data['price_per_bath'] = all_data['price'] / bath\n",
    "# Half baths are not interesting\n",
    "# See https://www.kaggle.com/arnaldcat/two-sigma-connect-rental-listing-inquiries/a-proxy-for-sqft-and-the-interest-on-1-2-baths/notebook\n",
    "all_data['half_bath'] = all_data['bathrooms'] == all_data['bathrooms'] // 1\n",
    "\n",
    "all_data['rooms'] = all_data['bathrooms'] * 0.5 + all_data['bedrooms']\n",
    "all_data['price_per_room'] = all_data['price'] / all_data['rooms']\n",
    "\n",
    "print('Create ratios')\n",
    "median_list = ['bedrooms', 'bathrooms', 'building_id', 'rooms', 'neighborhoods']\n",
    "for col in median_list:\n",
    "    median_price = all_data[[col, 'price']].groupby(col)['price'].median()\n",
    "    median_price = median_price[all_data[col]].values.astype(float)\n",
    "    all_data['median_' + col] = median_price\n",
    "    all_data['ratio_' + col] = all_data['price'] / median_price\n",
    "    all_data['median_' + col] = np.log(all_data['median_' + col].values)\n",
    "\n",
    "#print('Additional medians and ratios')\n",
    "median_list = [c for c in all_data.columns if c.startswith('median_')]\n",
    "all_data['median_mean'] = all_data[median_list].mean(axis=1)\n",
    "ratio_list = [c for c in all_data.columns if c.startswith('ratio_')]\n",
    "all_data['ratio_mean'] = all_data[ratio_list].mean(axis=1)\n",
    "    \n",
    "print('Normalize the price')\n",
    "all_data['price'] = np.log(all_data['price'].values)\n",
    "\n",
    "print('Building counts')\n",
    "bldg_count = all_data['building_id'].value_counts()\n",
    "bldg_count['0'] = 0\n",
    "all_data['bldg_count'] = np.log1p(bldg_count[all_data['building_id']].values)\n",
    "all_data['zero_bldg'] = all_data['building_id']=='0'\n",
    "\n",
    "print('Manager counts')\n",
    "mgr_count = all_data['manager_id'].value_counts()\n",
    "all_data['mgr_count'] = np.log1p(mgr_count[all_data['manager_id']].values)\n",
    "\n",
    "#Scale features\n",
    "scaler = StandardScaler()\n",
    "cols = [c for c in all_data.columns]\n",
    "scale_keywords = ['price', 'count', 'ratio', '_to_', \n",
    "                  'day_', 'hour', 'median', 'longitude', 'latitude']\n",
    "scale_list = [c for c in cols if any(w in c for w in scale_keywords)]\n",
    "print('Scaling features:', scale_list)\n",
    "all_data[scale_list] = scaler.fit_transform(all_data[scale_list].astype(float))\n",
    "\n",
    "print('Create dummies')\n",
    "mask = all_data['bathrooms'] > 3\n",
    "all_data.loc[mask, 'bathrooms'] = 4\n",
    "mask = all_data['bedrooms'] >= 5\n",
    "all_data.loc[mask, 'bedrooms'] = 5\n",
    "mask = all_data['rooms'] >= 6\n",
    "all_data.loc[mask, 'rooms'] = 6\n",
    "cat_cols = ['bathrooms', 'bedrooms', 'month', 'weekday', 'rooms', \n",
    "            'neighborhoods']\n",
    "for col in cat_cols:\n",
    "    dummy = pd.get_dummies(all_data[col], prefix=col)\n",
    "    dummy = dummy.astype(bool) \n",
    "    all_data = all_data.join(dummy)\n",
    "all_data.drop(cat_cols, axis=1, inplace=True)\n",
    "\n",
    "print('Drop columns')\n",
    "drop_cols = ['description', 'photos', 'display_address', 'street_address', \n",
    "             'features', 'created', 'building_id', 'manager_id', \n",
    "             'longitude', 'latitude', 'time_stamp', 'img_date'\n",
    "             ]\n",
    "             \n",
    "all_data.drop(drop_cols, axis=1, inplace=True)\n",
    "\n",
    "data_columns = all_data.columns.tolist()\n",
    "data_columns.remove(label_column)\n",
    "\n",
    "mask = all_data[label_column].isnull()\n",
    "train = all_data[~mask].copy()\n",
    "test = all_data[mask].copy()\n",
    "\n",
    "elapsed = (time.time() - start_time)\n",
    "print('Data loaded and prepared in:', timedelta(seconds=elapsed))\n",
    "\n",
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128,  \n",
    "                    activation='relu',\n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=l2(0.000025),\n",
    "                    kernel_constraint=max_norm(2.0),\n",
    "                    input_shape = (len(data_columns),),))\n",
    "#    model.add(Dropout(0.25))\n",
    "    model.add(PReLU())\n",
    "    \n",
    "#    model.add(Dense(64,  \n",
    "#                    activation='softplus',\n",
    "#                    kernel_initializer='he_normal',\n",
    "#                    kernel_regularizer=l2(0.000025),\n",
    "#                    kernel_constraint=max_norm(2.0),\n",
    "#                    ))\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    \n",
    "    model.add(Dense(16, \n",
    "                    activation='relu', \n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=l2(0.000025),\n",
    "                    #kernel_constraint=max_norm(2.0)\n",
    "                    ))\n",
    "#    model.add(Dropout(0.1))\n",
    "    \n",
    "    model.add(Dense(32,\n",
    "                    activation='softplus', \n",
    "                    kernel_initializer='he_normal',\n",
    "                    kernel_regularizer=l2(0.00005),\n",
    "                    kernel_constraint=max_norm(2.0)\n",
    "                    ))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Dense(units=num_classes, \n",
    "                    activation='softmax', \n",
    "                    kernel_initializer='he_normal',\n",
    "                    ))\n",
    "#    opt = optimizers.Adadelta(lr=1)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', \n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy']\n",
    "                  )\n",
    "    return(model)\n",
    "\n",
    "model = nn_model()\n",
    "X = train[data_columns].values\n",
    "y = train[label_column].values\n",
    "model.fit(X, y, epochs = 100, batch_size=1024, verbose = 2)\n",
    "\n",
    "train_pred = model.predict_proba(X)\n",
    "#Normalize the predictions\n",
    "pred_sum = train_pred.sum(axis=1)\n",
    "train_pred = train_pred / pred_sum[:, None]\n",
    "score = log_loss(y, train_pred)\n",
    "print('Score:', score)\n",
    "\n",
    "test_pred = model.predict_proba(test[data_columns].values)\n",
    "#Normalize the predictions\n",
    "pred_sum = test_pred.sum(axis=1)\n",
    "test_pred = test_pred / pred_sum[:, None]\n",
    "test_out = pd.DataFrame(test_pred, columns = ['high', 'medium', 'low'], index=test.index)\n",
    "test_out.to_csv('simple_nn_with_leak.csv')\n",
    "\n",
    "elapsed = (time.time() - start_time)\n",
    "print('Completed in:', timedelta(seconds=elapsed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('simple_nn_with_leak.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn\n",
    "\n",
    "train_df = pd.read_json(\"train.json\")\n",
    "test_df = pd.read_json(\"test.json\")\n",
    "\n",
    "\n",
    "\n",
    "image_date = pd.read_csv(\"input/listing_image_time.csv\")\n",
    "\n",
    "# rename columns so you can join tables later on\n",
    "image_date.columns = [\"listing_id\", \"time_stamp\"]\n",
    "\n",
    "# reassign the only one timestamp from April, all others from Oct/Nov\n",
    "image_date.loc[80240,\"time_stamp\"] = 1478129766 \n",
    "\n",
    "image_date[\"img_date\"]                  = pd.to_datetime(image_date[\"time_stamp\"], unit=\"s\")\n",
    "image_date[\"img_days_passed\"]           = (image_date[\"img_date\"].max() - image_date[\"img_date\"]).astype(\"timedelta64[D]\").astype(int)\n",
    "image_date[\"img_date_month\"]            = image_date[\"img_date\"].dt.month\n",
    "image_date[\"img_date_week\"]             = image_date[\"img_date\"].dt.week\n",
    "image_date[\"img_date_day\"]              = image_date[\"img_date\"].dt.day\n",
    "image_date[\"img_date_dayofweek\"]        = image_date[\"img_date\"].dt.dayofweek\n",
    "image_date[\"img_date_dayofyear\"]        = image_date[\"img_date\"].dt.dayofyear\n",
    "image_date[\"img_date_hour\"]             = image_date[\"img_date\"].dt.hour\n",
    "image_date[\"img_date_monthBeginMidEnd\"] = image_date[\"img_date_day\"].apply(lambda x: 1 if x<10 else 2 if x<20 else 3)\n",
    "\n",
    "YourDF = pd.merge(YourDF, image_date, on=\"listing_id\", how=\"left\")\n",
    "\n",
    "\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=321, num_rounds=1800):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.021\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model\n",
    "\n",
    "test_df[\"bathrooms\"].loc[19671] = 1.5\n",
    "test_df[\"bathrooms\"].loc[22977] = 2.0\n",
    "test_df[\"bathrooms\"].loc[63719] = 2.0\n",
    "train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "\n",
    "train_df['half_bathrooms'] = train_df[\"bathrooms\"] - train_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "test_df['half_bathrooms'] = test_df[\"bathrooms\"] - test_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "\n",
    "train_df[\"price_t\"] =train_df[\"price\"]/train_df[\"bedrooms\"]\n",
    "test_df[\"price_t\"] = test_df[\"price\"]/test_df[\"bedrooms\"] \n",
    "\n",
    "train_df[\"room_sum\"] = train_df[\"bedrooms\"]+train_df[\"bathrooms\"] \n",
    "test_df[\"room_sum\"] = test_df[\"bedrooms\"]+test_df[\"bathrooms\"] \n",
    "\n",
    "train_df['price_per_room'] = train_df['price']/train_df['room_sum']\n",
    "test_df['price_per_room'] = test_df['price']/test_df['room_sum']\n",
    "\n",
    "train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "\n",
    "train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "\n",
    "train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "train_df[\"created_year\"] = train_df[\"created\"].dt.year\n",
    "test_df[\"created_year\"] = test_df[\"created\"].dt.year\n",
    "train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "\n",
    "train_df[\"created_weekday\"] = train_df[\"created\"].dt.weekday\n",
    "test_df[\"created_weekday\"] = test_df[\"created\"].dt.weekday\n",
    "train_df[\"created_week\"] = train_df[\"created\"].dt.week\n",
    "test_df[\"created_week\"] = test_df[\"created\"].dt.week\n",
    "\n",
    "train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "vals = train_df['pos'].value_counts()\n",
    "dvals = vals.to_dict()\n",
    "train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "features_to_use=[\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\",\"price_t\",\"price_per_room\", \"logprice\", \"density\", \"half_bathrooms\",\n",
    "\"num_photos\", \"num_features\", \"num_description_words\",\"listing_id\", \"created_year\", \"created_month\", \"created_day\", \"created_hour\", \"created_week\", \"created_weekday\"]\n",
    "\n",
    "index=list(range(train_df.shape[0]))\n",
    "random.shuffle(index)\n",
    "a=[np.nan]*len(train_df)\n",
    "b=[np.nan]*len(train_df)\n",
    "c=[np.nan]*len(train_df)\n",
    "\n",
    "for i in range(5):\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "    \n",
    "    test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "    train_index=list(set(index).difference(test_index))\n",
    "    \n",
    "    for j in train_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "            \n",
    "    for j in test_index:\n",
    "        temp=train_df.iloc[j]\n",
    "        if sum(building_level[temp['manager_id']])!=0:\n",
    "            a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "            b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "            c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "            \n",
    "train_df['manager_level_low']=a\n",
    "train_df['manager_level_medium']=b\n",
    "train_df['manager_level_high']=c\n",
    "\n",
    "a=[]\n",
    "b=[]\n",
    "c=[]\n",
    "building_level={}\n",
    "for j in train_df['manager_id'].values:\n",
    "    building_level[j]=[0,0,0]\n",
    "\n",
    "for j in range(train_df.shape[0]):\n",
    "    temp=train_df.iloc[j]\n",
    "    if temp['interest_level']=='low':\n",
    "        building_level[temp['manager_id']][0]+=1\n",
    "    if temp['interest_level']=='medium':\n",
    "        building_level[temp['manager_id']][1]+=1\n",
    "    if temp['interest_level']=='high':\n",
    "        building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "for i in test_df['manager_id'].values:\n",
    "    if i not in building_level.keys():\n",
    "        a.append(np.nan)\n",
    "        b.append(np.nan)\n",
    "        c.append(np.nan)\n",
    "    else:\n",
    "        a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "        b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "        c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "test_df['manager_level_low']=a\n",
    "test_df['manager_level_medium']=b\n",
    "test_df['manager_level_high']=c\n",
    "\n",
    "features_to_use.append('manager_level_low') \n",
    "features_to_use.append('manager_level_medium') \n",
    "features_to_use.append('manager_level_high')\n",
    "\n",
    "categorical = [\"street_address\", \"display_address\", \"manager_id\", \"building_id\"]\n",
    "for f in categorical:\n",
    "        if train_df[f].dtype=='object':\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train_df[f].values) + list(test_df[f].values))\n",
    "            train_df[f] = lbl.transform(list(train_df[f].values))\n",
    "            test_df[f] = lbl.transform(list(test_df[f].values))\n",
    "            features_to_use.append(f)\n",
    "\n",
    "\n",
    "train_df['features'] = train_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "test_df['features'] = test_df[\"features\"].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "\n",
    "tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "tr_sparse = tfidf.fit_transform(train_df[\"features\"])\n",
    "te_sparse = tfidf.transform(test_df[\"features\"])\n",
    "\n",
    "train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()\n",
    "test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()\n",
    "\n",
    "train_X = train_X.toarray()\n",
    "test_X = test_X.toarray()\n",
    "\n",
    "\n",
    "\n",
    "target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = train_X\n",
    "index = ['Row'+str(i) for i in range(1, len(values)+1)]\n",
    "\n",
    "train_X = pd.DataFrame(values, index=index, dtype=\"int64\")\n",
    "\n",
    "values = test_X\n",
    "index = ['Row'+str(i) for i in range(1, len(values)+1)]\n",
    "\n",
    "test_X = pd.DataFrame(values, index=index, dtype=\"int64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = train_X.fillna(train_X.mean())\n",
    "test_X = test_X.fillna(test_X.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#test_X.isnull().sum().sum()\n",
    "np.isnan(train_X).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "print(test_X.shape)\n",
    "print(train_X.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=500, criterion='entropy', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, oob_score=True, n_jobs=-1, random_state=None, verbose=0, warm_start=False, class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(train_X,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = model.predict_proba(test_X)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub[\"listing_id\"] = test_df[\"listing_id\"]\n",
    "for label in [\"high\", \"medium\", \"low\"]:\n",
    "    sub[label] = y[:, target_num_map[label]]\n",
    "sub.to_csv(\"submission_rf_base.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('submission_rf_base.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StackNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!wget https://github.com/kaz-Anova/StackNet/blob/master/StackNet.jar\n",
    "!curl -O https://github.com/kaz-Anova/StackNet/raw/master/Stackent_source.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!java -version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "    #load the data . There is potential leakage regarding the listing_ids (as they seem to be correlated with time)\n",
    "    train_file = \"train.json\"\n",
    "    test_file = \"test.json\"\n",
    "    train_df = pd.read_json(train_file)\n",
    "    test_df = pd.read_json(test_file)\n",
    "    print(train_df.shape)\n",
    "    print(test_df.shape)\n",
    "    features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\", \"per_sqft\",\n",
    "                       \"img_days_passed\", \"img_date_month\", \"img_date_week\", \"img_date_day\", \"img_date_dayofweek\", \"img_date_dayofyear\", \"img_date_hour\", \"img_date_monthBeginMidEnd\", \"logprice\", \"num_photos\", \"half_bathrooms\", \"num_description_words\", \"num_price_by_furniture\", \"price_latitue\", \"price_longtitude\", \"num_furniture\", \"created_month\", \"created_day\", \"created_weekday\", \"created_week\", \"created_hour\", \"total_days\", \"diff_rank\", \"density\",\n",
    "                       'num_rho', 'num_phi',\n",
    "                       'num_rot15_X', 'num_rot15_Y', 'num_rot30_X', 'num_rot30_Y',\n",
    "                       'num_rot45_X', 'num_rot45_Y', 'num_rot60_X', 'num_rot60_Y',\n",
    "                       'num_cap_share', 'num_nr_of_lines', 'num_redacted', 'num_email',\n",
    "                       'num_phone_nr']\n",
    "\n",
    "    \n",
    "    image_date = pd.read_csv(\"listing_image_time0.csv\")\n",
    "\n",
    "    # rename columns so you can join tables later on\n",
    "    image_date.columns = [\"listing_id\", \"time_stamp\"]\n",
    "\n",
    "    # reassign the only one timestamp from April, all others from Oct/Nov\n",
    "    image_date.loc[80240,\"time_stamp\"] = 1478129766 \n",
    "\n",
    "    image_date[\"img_date\"]                  = pd.to_datetime(image_date[\"time_stamp\"], unit=\"s\")\n",
    "    image_date[\"img_days_passed\"]           = (image_date[\"img_date\"].max() - image_date[\"img_date\"]).astype(\"timedelta64[D]\").astype(int)\n",
    "    image_date[\"img_date_month\"]            = image_date[\"img_date\"].dt.month\n",
    "    image_date[\"img_date_week\"]             = image_date[\"img_date\"].dt.week\n",
    "    image_date[\"img_date_day\"]              = image_date[\"img_date\"].dt.day\n",
    "    image_date[\"img_date_dayofweek\"]        = image_date[\"img_date\"].dt.dayofweek\n",
    "    image_date[\"img_date_dayofyear\"]        = image_date[\"img_date\"].dt.dayofyear\n",
    "    image_date[\"img_date_hour\"]             = image_date[\"img_date\"].dt.hour\n",
    "    image_date[\"img_date_monthBeginMidEnd\"] = image_date[\"img_date_day\"].apply(lambda x: 1 if x<10 else 2 if x<20 else 3)\n",
    "\n",
    "    train_df = pd.merge(train_df, image_date, on=\"listing_id\", how=\"left\")\n",
    "    test_df = pd.merge(test_df, image_date, on=\"listing_id\", how=\"left\")\n",
    "    \n",
    "\n",
    "    import math\n",
    "    def cart2rho(x, y):\n",
    "        rho = np.sqrt(x**2 + y**2)\n",
    "        return rho\n",
    "\n",
    "\n",
    "    def cart2phi(x, y):\n",
    "        phi = np.arctan2(y, x)\n",
    "        return phi\n",
    "\n",
    "\n",
    "    def rotation_x(row, alpha):\n",
    "        x = row['latitude']\n",
    "        y = row['longitude']\n",
    "        return x*math.cos(alpha) + y*math.sin(alpha)\n",
    "\n",
    "\n",
    "    def rotation_y(row, alpha):\n",
    "        x = row['latitude']\n",
    "        y = row['longitude']\n",
    "        return y*math.cos(alpha) - x*math.sin(alpha)\n",
    "\n",
    "\n",
    "    def add_rotation(degrees, df):\n",
    "        namex = \"rot\" + str(degrees) + \"_X\"\n",
    "        namey = \"rot\" + str(degrees) + \"_Y\"\n",
    "\n",
    "        df['num_' + namex] = df.apply(lambda row: rotation_x(row, math.pi/(180/degrees)), axis=1)\n",
    "        df['num_' + namey] = df.apply(lambda row: rotation_y(row, math.pi/(180/degrees)), axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def operate_on_coordinates(tr_df, te_df):\n",
    "        for df in [tr_df, te_df]:\n",
    "            #polar coordinates system\n",
    "            df[\"num_rho\"] = df.apply(lambda x: cart2rho(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "            df[\"num_phi\"] = df.apply(lambda x: cart2phi(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "            #rotations\n",
    "            for angle in [15,30,45,60]:\n",
    "                df = add_rotation(angle, df)\n",
    "\n",
    "        return tr_df, te_df\n",
    "\n",
    "    train_df, test_df = operate_on_coordinates(train_df, test_df)\n",
    "    \n",
    "    import re\n",
    "\n",
    "    def cap_share(x):\n",
    "        return sum(1 for c in x if c.isupper())/float(len(x)+1)\n",
    "\n",
    "    for df in [train_df, test_df]:\n",
    "        # do you think that users might feel annoyed BY A DESCRIPTION THAT IS SHOUTING AT THEM?\n",
    "        df['num_cap_share'] = df['description'].apply(cap_share)\n",
    "\n",
    "        # how long in lines the desc is?\n",
    "        df['num_nr_of_lines'] = df['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "\n",
    "        # is the description redacted by the website?        \n",
    "        df['num_redacted'] = 0\n",
    "        df['num_redacted'].ix[df['description'].str.contains('website_redacted')] = 1\n",
    "\n",
    "\n",
    "        # can we contact someone via e-mail to ask for the details?\n",
    "        df['num_email'] = 0\n",
    "        df['num_email'].ix[df['description'].str.contains('@')] = 1\n",
    "\n",
    "        #and... can we call them?\n",
    "\n",
    "        reg = re.compile(\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", re.S)\n",
    "        def try_and_find_nr(description):\n",
    "            if reg.match(description) is None:\n",
    "                return 0\n",
    "            return 1\n",
    "\n",
    "        df['num_phone_nr'] = df['description'].apply(try_and_find_nr)\n",
    "    \n",
    "    \n",
    "    test_df[\"bathrooms\"].loc[19671] = 1.5\n",
    "    test_df[\"bathrooms\"].loc[22977] = 2.0\n",
    "    test_df[\"bathrooms\"].loc[63719] = 2.0\n",
    "    train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "    train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "    test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "    train_df[\"per_sqft\"] = train_df[\"price\"]/(1+train_df[\"bedrooms\"].clip(1,4)+train_df[\"bathrooms\"].clip(0,2))\n",
    "    test_df[\"per_sqft\"] = test_df[\"price\"]/(1+test_df[\"bedrooms\"].clip(1,4)+test_df[\"bathrooms\"].clip(0,2))\n",
    "\n",
    "    # count of photos #\n",
    "    train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "    test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "    \n",
    "    # count of \"features\" #\n",
    "    train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "    test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "    train_df[\"listing_id\"] = train_df[\"listing_id\"] - 68119576.0\n",
    "    test_df[\"listing_id\"] =  test_df[\"listing_id\"] - 68119576.0\n",
    "    \n",
    "    \n",
    "    train_df['half_bathrooms'] = train_df[\"bathrooms\"] - train_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "    test_df['half_bathrooms'] = test_df[\"bathrooms\"] - test_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "\n",
    "    # count of words present in description column #\n",
    "    train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    " \n",
    "    train_df[\"num_price_by_furniture\"] = (train_df[\"price\"])/ (train_df[\"bathrooms\"] + train_df[\"bedrooms\"] + 1.0)\n",
    "    test_df[\"num_price_by_furniture\"] =  (test_df[\"price\"])/ (test_df[\"bathrooms\"] + test_df[\"bedrooms\"] +  1.0)\n",
    "    \n",
    "    train_df[\"price_latitue\"] = (train_df[\"price\"])/ (train_df[\"latitude\"]+1.0)\n",
    "    test_df[\"price_latitue\"] =  (test_df[\"price\"])/ (test_df[\"latitude\"]+1.0)\n",
    "    \n",
    "    train_df[\"price_longtitude\"] = (train_df[\"price\"])/ (train_df[\"longitude\"]-1.0)\n",
    "    test_df[\"price_longtitude\"] =  (test_df[\"price\"])/ (test_df[\"longitude\"]-1.0)  \n",
    "\n",
    "    train_df[\"num_furniture\"] =  train_df[\"bathrooms\"] + train_df[\"bedrooms\"] \n",
    "    test_df[\"num_furniture\"] =   test_df[\"bathrooms\"] + test_df[\"bedrooms\"] \n",
    "    \n",
    "    train_df[\"num_furniture\"] = train_df[\"num_furniture\"].apply(lambda x:  str(x) if float(x)<9.5 else '10')\n",
    "    test_df[\"num_furniture\"] = test_df[\"num_furniture\"].apply(lambda x:  str(x) if float(x)<9.5 else '10')\n",
    "            \n",
    "    # convert the created column to datetime object so as to extract more features \n",
    "    train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "    test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "    \n",
    "    # Let us extract some features like year, month, day, hour from date columns #\n",
    "    train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "    test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "    train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "    test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "    train_df[\"created_weekday\"] = train_df[\"created\"].dt.weekday\n",
    "    test_df[\"created_weekday\"] = test_df[\"created\"].dt.weekday\n",
    "    train_df[\"created_week\"] = train_df[\"created\"].dt.week\n",
    "    test_df[\"created_week\"] = test_df[\"created\"].dt.week               \n",
    "    train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "    test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "    train_df[\"total_days\"] =   (train_df[\"created_month\"] -4.0)*30 + train_df[\"created_day\"] +  train_df[\"created_hour\"] /25.0\n",
    "    test_df[\"total_days\"] =(test_df[\"created_month\"] -4.0)*30 + test_df[\"created_day\"] +  test_df[\"created_hour\"] /25.0        \n",
    "    train_df[\"diff_rank\"]= train_df[\"total_days\"]/train_df[\"listing_id\"]\n",
    "    test_df[\"diff_rank\"]= test_df[\"total_days\"]/test_df[\"listing_id\"]\n",
    "\n",
    "    \n",
    "    train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "    test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "    vals = train_df['pos'].value_counts()\n",
    "    dvals = vals.to_dict()\n",
    "    train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "    test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "        \n",
    "    \n",
    "    categorical = [ \"display_address\", \"manager_id\", \"building_id\",\"street_address\",\"num_furniture\"]#,\"num_furniture\",\"latitude_binned\"]#\"\", \"\",\"street_address\"\n",
    "    lencat=len(categorical)\n",
    "\n",
    "    for f in range (0,lencat):\n",
    "        for s in range (f+1,lencat): \n",
    "            train_df[categorical[f] + \"_\" +categorical[s]] =train_df[categorical[f]]+\"_\" + train_df[categorical[s]]\n",
    "            test_df[categorical[f] + \"_\" +categorical[s]] =test_df[categorical[f]]+\"_\" + test_df[categorical[s]]            \n",
    "            categorical.append(categorical[f] + \"_\" +categorical[s])\n",
    "       \n",
    "    # adding all these new features to use list #\n",
    "    features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\",\"total_days\",\"diff_rank\",#\"listing_rank\",\"total_days_rank\",\n",
    "    \"num_price_by_furniture\",\"price_latitue\",\"price_longtitude\"])#,\"price_latitue_longtitude\"]) \"created_year\", #,\"num_description_length\"\n",
    "    result = pd.concat([train_df,test_df])\n",
    "\n",
    "    for f in categorical:\n",
    "            if train_df[f].dtype=='object':\n",
    "\n",
    "                cases=defaultdict(int)\n",
    "                temp=np.array(result[f]).tolist()\n",
    "                for k in temp:\n",
    "                    cases[k]+=1\n",
    "                print (f, len(cases)) \n",
    "                \n",
    "                train_df[f]=train_df[f].apply(lambda x: cases[x])\n",
    "                test_df[f]=test_df[f].apply(lambda x: cases[x])               \n",
    "                \n",
    "                features_to_use.append(f)  \n",
    "\n",
    "    train_df['features'] =  train_df['features'].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "    test_df['features'] =test_df['features'].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))  \n",
    "\n",
    "    train_df['description'] =  train_df['description'].apply(lambda x: str(x).encode('utf-8') if len(x)>2 else \"nulldesc\") \n",
    "    test_df['description'] =test_df['description'].apply(lambda x: str(x).encode('utf-8') if len(x)>2 else \"nulldesc\") \n",
    "    \n",
    "    tfidfdesc=TfidfVectorizer(min_df=20, max_features=50, strip_accents='unicode',lowercase =True,\n",
    "                        analyzer='word', token_pattern=r'\\w{16,}', ngram_range=(1, 2), use_idf=False,smooth_idf=False, \n",
    "    sublinear_tf=True, stop_words = 'english')  \n",
    "    \n",
    "    print(train_df[\"features\"].head())\n",
    "       \n",
    "    tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "    \n",
    "    te_sparse = tfidf.fit_transform(test_df[\"features\"])  \n",
    "    tr_sparse = tfidf.transform(train_df[\"features\"])   \n",
    "\n",
    "    te_sparsed = tfidfdesc. fit_transform(test_df[\"description\"])  \n",
    "    tr_sparsed = tfidfdesc.transform(train_df[\"description\"])\n",
    "    print(features_to_use)\n",
    "    \n",
    "    train_df[features_to_use] = train_df[features_to_use].fillna(train_df[features_to_use].mean())\n",
    "    test_df[features_to_use] = test_df[features_to_use].fillna(test_df[features_to_use].mean())\n",
    "\n",
    "    train_df[['manager_level_low', 'manager_level_medium', 'manager_level_high']] = train_df[['manager_level_low', 'manager_level_medium', 'manager_level_high']].fillna(0)\n",
    "    test_df[['manager_level_low', 'manager_level_medium', 'manager_level_high']] = test_df[['manager_level_low', 'manager_level_medium', 'manager_level_high']].fillna(0)    \n",
    "\n",
    "    train_X = sparse.hstack([train_df[features_to_use], tr_sparse,tr_sparsed]).tocsr()#\n",
    "    test_X = sparse.hstack([test_df[features_to_use], te_sparse,te_sparsed]).tocsr()#\n",
    "    \n",
    "    target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "    train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "    ids= test_df.listing_id.values\n",
    "    print(train_X.shape, test_X.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = train_X.toarray()\n",
    "\n",
    "X[np.isnan(X)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(train_X.toarray()))[:,:]\n",
    "#np.argwhere(np.isinf(train_X.toarray()))[:,:]\n",
    "\n",
    "#list(map(tuple, np.where(np.isnan(train_X.toarray()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.sum(np.isnan(train_X.toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features_to_use[46]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameter name : train_file value :  train_stacknet.csv\n",
      "parameter name : test_file value :  test_stacknet.csv\n",
      "parameter name : params value :  paramssimplev1.txt\n",
      "parameter name : pred_file value :  sigma_stack_pred.csv\n",
      "parameter name : test_target value :  true\n",
      "parameter name : verbose value :  true\n",
      "parameter name : threads value :  3\n",
      "parameter name : stackdata value :  false\n",
      "parameter name : folds value :  5\n",
      "parameter name : seed value :  1\n",
      "parameter name : metric value :  logloss\n",
      " Completed: 5.00 % \n",
      " Completed: 10.00 % \n",
      " Completed: 15.00 % \n",
      " Completed: 20.00 % \n",
      " Completed: 24.99 % \n",
      " Completed: 29.99 % \n",
      " Completed: 34.99 % \n",
      " Completed: 39.99 % \n",
      " Completed: 44.99 % \n",
      " Completed: 49.99 % \n",
      " Completed: 54.99 % \n",
      " Completed: 59.99 % \n",
      " Completed: 64.98 % \n",
      " Completed: 69.98 % \n",
      " Completed: 74.98 % \n",
      " Completed: 79.98 % \n",
      " Completed: 84.98 % \n",
      " Completed: 89.98 % \n",
      " Completed: 94.98 % \n",
      " Completed: 99.98 % \n",
      " Loaded File: train_stacknet.csv\n",
      " Total rows in the file: 49352\n",
      " Total columns in the file: 329\n",
      " Weighted variable : -1 counts: 0\n",
      " Int Id variable : -1 str id: -1 counts: 0\n",
      " Target Variables  : 1 values : [0]\n",
      " Actual columns number  : 328\n",
      " Number of Skipped rows   : 0\n",
      " Actual Rows (removing the skipped ones)  : 49352\n",
      "Loaded dense train data with 49352 and columns 328\n",
      " loaded data in : 4.690000\n",
      " Level: 1 dimensionality: 21\n",
      " Starting cross validation \n",
      "Fitting model: 1\n",
      "Fitting model: 2\n",
      "Fitting model: 3\n",
      " logloss : 0.5296721557463386\n",
      " logloss : 0.517875456647624\n",
      " logloss : 0.5257356883098425\n",
      "Fitting model: 4\n",
      "Fitting model: 5\n",
      "Fitting model: 6\n",
      " logloss : 0.5219729828709598\n",
      " logloss : 0.5145627981959118\n",
      " logloss : 0.5208836507113833\n",
      "Fitting model: 7\n",
      "Fitting model: 8\n",
      "Fitting model: 9\n",
      " rmse : 0.45538708369979625\n",
      " rmse : 0.4541752180415414\n",
      " rmse : 0.6511152984762575\n",
      "Done with fold: 1/5\n",
      "Fitting model: 1\n",
      "Fitting model: 2\n",
      "Fitting model: 3\n",
      " logloss : 0.5199587422775901\n",
      " logloss : 0.5119965235048005\n",
      " logloss : 0.5190075120442383\n",
      "Fitting model: 4\n",
      "Fitting model: 5\n",
      "Fitting model: 6\n",
      " logloss : 0.515441913159958\n",
      " logloss : 0.5083755445327885\n",
      " logloss : 0.5152288799554448\n",
      "Fitting model: 7\n",
      "Fitting model: 8\n",
      "Fitting model: 9\n",
      " rmse : 0.4510532161935566\n",
      " rmse : 0.4496070030956014\n",
      " rmse : 0.6133072106285415\n",
      "Done with fold: 2/5\n",
      "Fitting model: 1\n",
      "Fitting model: 2\n",
      "Fitting model: 3\n",
      " logloss : 0.5226587617417575\n",
      " logloss : 0.5104416523091757\n",
      " logloss : 0.5211049310064174\n",
      "Fitting model: 4\n",
      "Fitting model: 5\n",
      "Fitting model: 6\n",
      " logloss : 0.5164867189547185\n",
      " logloss : 0.50702559461228\n",
      " logloss : 0.513855341307681\n",
      "Fitting model: 7\n",
      "Fitting model: 8\n",
      "Fitting model: 9\n",
      " rmse : 0.4479204252213597\n",
      " rmse : 0.44807883062661985\n",
      " rmse : 0.8638911850448924\n",
      "Done with fold: 3/5\n",
      "Fitting model: 1\n",
      "Fitting model: 2\n",
      "Fitting model: 3\n",
      " logloss : 0.5281862339523226\n",
      " logloss : 0.5120068913226014\n",
      " logloss : 0.523969025986451\n",
      "Fitting model: 4\n",
      "Fitting model: 5\n",
      "Fitting model: 6\n",
      " logloss : 0.5194451417801242\n",
      " logloss : 0.5084201866698114\n",
      " logloss : 0.5144753868128715\n",
      "Fitting model: 7\n",
      "Fitting model: 8\n",
      "Fitting model: 9\n",
      " rmse : 0.4485943718804409\n",
      " rmse : 0.44696266732495815\n",
      " rmse : 0.5447353985931129\n",
      "Done with fold: 4/5\n",
      "Fitting model: 1\n",
      "Fitting model: 2\n",
      "Fitting model: 3\n",
      " logloss : 0.5252928792536764\n",
      " logloss : 0.5153847819311682\n",
      " logloss : 0.5224130497125811\n",
      "Fitting model: 4\n",
      "Fitting model: 5\n",
      "Fitting model: 6\n",
      " logloss : 0.517489688985623\n",
      " logloss : 0.509669614664994\n",
      " logloss : 0.5146884454732998\n",
      "Fitting model: 7\n",
      "Fitting model: 8\n",
      "Fitting model: 9\n",
      " rmse : 0.44572947029313037\n",
      " rmse : 0.44533909150698875\n",
      " rmse : 0.5366975432973558\n",
      "Done with fold: 5/5\n",
      " Level: 1 start output modelling \n",
      "Fitting model : 1\n",
      "Fitting model : 2\n",
      "Fitting model : 3\n",
      "Fitting model : 4\n",
      "Fitting model : 5\n",
      "Fitting model : 6\n",
      "Fitting model : 7\n",
      "Fitting model : 8\n",
      "Fitting model : 9\n",
      "Completed level: 1 out of 2\n",
      " Level: 2 dimensionality: 3\n",
      " Starting cross validation \n",
      " Level: 2 start output modelling \n",
      "Fitting model : 1\n",
      "Completed level: 2 out of 2\n",
      " modelling lasted : 3510.985000\n",
      " Completed: 5.00 % \n",
      " Completed: 10.00 % \n",
      " Completed: 15.00 % \n",
      " Completed: 19.99 % \n",
      " Completed: 24.99 % \n",
      " Completed: 29.99 % \n",
      " Completed: 34.99 % \n",
      " Completed: 39.99 % \n",
      " Completed: 44.99 % \n",
      " Completed: 49.99 % \n",
      " Completed: 54.99 % \n",
      " Completed: 59.98 % \n",
      " Completed: 64.98 % \n",
      " Completed: 69.98 % \n",
      " Completed: 74.98 % \n",
      " Completed: 79.98 % \n",
      " Completed: 84.98 % \n",
      " Completed: 89.98 % \n",
      " Completed: 94.98 % \n",
      " Completed: 99.97 % \n",
      " Loaded File: test_stacknet.csv\n",
      " Total rows in the file: 74659\n",
      " Total columns in the file: 329\n",
      " Weighted variable : -1 counts: 0\n",
      " Int Id variable : -1 str id: -1 counts: 0\n",
      " Target Variables  : 1 values : [0]\n",
      " Actual columns number  : 328\n",
      " Number of Skipped rows   : 0\n",
      " Actual Rows (removing the skipped ones)  : 74659\n",
      "Loaded dense test data with 74659 and columns 328\n",
      " loading test data lasted : 5.124000\n",
      " Completed: 5.00 % \n",
      " Completed: 10.00 % \n",
      " Completed: 15.00 % \n",
      " Completed: 19.99 % \n",
      " Completed: 24.99 % \n",
      " Completed: 29.99 % \n",
      " Completed: 34.99 % \n",
      " Completed: 39.99 % \n",
      " Completed: 44.99 % \n",
      " Completed: 49.99 % \n",
      " Completed: 54.99 % \n",
      " Completed: 59.98 % \n",
      " Completed: 64.98 % \n",
      " Completed: 69.98 % \n",
      " Completed: 74.98 % \n",
      " Completed: 79.98 % \n",
      " Completed: 84.98 % \n",
      " Completed: 89.98 % \n",
      " Completed: 94.98 % \n",
      " Completed: 99.97 % \n",
      " predicting on test data lasted : 36.660000\n",
      "Metric could not be calculated on the test \n",
      " The whole StackNet procedure lasted: 3561.712000\n"
     ]
    }
   ],
   "source": [
    "!java -Xmx3048m -jar StackNet0.jar train train_file=train_stacknet.csv test_file=test_stacknet.csv params=paramssimplev1.txt pred_file=sigma_stack_pred.csv test_target=true verbose=true Threads=3 stackdata=false folds=5 seed=1 metric=logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(74659, 4)\n",
      "(74659, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "sigmastack_pred = pd.read_csv(\"sigma_stack_pred.csv\",header=None)\n",
    "test = pd.read_json(\"test.json\")\n",
    "sigmastack_pred.insert(0,'listing_id', test.listing_id)\n",
    "#test.listing_id.head(10)\n",
    "sigmastack_pred.head(10)\n",
    "print(sigmastack_pred.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='sigma_stack_pred.csv' target='_blank'>sigma_stack_pred.csv</a><br>"
      ],
      "text/plain": [
       "/resources/sigma_stack_pred.csv"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('sigma_stack_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49352, 15)\n",
      "(74659, 14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/pandas/core/indexing.py:132: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "display_address 16068\n",
      "manager_id 4399\n",
      "building_id 11635\n",
      "street_address 25766\n",
      "num_furniture 19\n",
      "display_address_manager_id 62997\n",
      "display_address_building_id 28492\n",
      "display_address_street_address 29336\n",
      "display_address_num_furniture 25623\n",
      "manager_id_building_id 58550\n",
      "manager_id_street_address 73650\n",
      "manager_id_num_furniture 15232\n",
      "building_id_street_address 29348\n",
      "building_id_num_furniture 19146\n",
      "street_address_num_furniture 37082\n",
      "0                                                     \n",
      "1    Doorman Elevator Fitness_Center Cats_Allowed D...\n",
      "2    Laundry_In_Building Dishwasher Hardwood_Floors...\n",
      "3                               Hardwood_Floors No_Fee\n",
      "4                                              Pre-War\n",
      "Name: features, dtype: object\n",
      "['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'per_sqft', 'img_days_passed', 'img_date_month', 'img_date_week', 'img_date_day', 'img_date_dayofweek', 'img_date_dayofyear', 'img_date_hour', 'img_date_monthBeginMidEnd', 'logprice', 'num_photos', 'half_bathrooms', 'num_description_words', 'num_price_by_furniture', 'price_latitue', 'price_longtitude', 'num_furniture', 'created_month', 'created_day', 'created_weekday', 'created_week', 'created_hour', 'total_days', 'diff_rank', 'density', 'num_rho', 'num_phi', 'num_rot15_X', 'num_rot15_Y', 'num_rot30_X', 'num_rot30_Y', 'num_rot45_X', 'num_rot45_Y', 'num_rot60_X', 'num_rot60_Y', 'num_cap_share', 'num_nr_of_lines', 'num_redacted', 'num_email', 'num_phone_nr', 'manager_level_low', 'manager_level_medium', 'manager_level_high', 'num_photos', 'num_features', 'num_description_words', 'created_month', 'created_day', 'listing_id', 'created_hour', 'total_days', 'diff_rank', 'num_price_by_furniture', 'price_latitue', 'price_longtitude', 'display_address', 'manager_id', 'building_id', 'street_address', 'num_furniture', 'display_address_manager_id', 'display_address_building_id', 'display_address_street_address', 'display_address_num_furniture', 'manager_id_building_id', 'manager_id_street_address', 'manager_id_num_furniture', 'building_id_street_address', 'building_id_num_furniture', 'street_address_num_furniture']\n",
      "(49352, 325) (74659, 325)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "    \n",
    "    #load the data . There is potential leakage regarding the listing_ids (as they seem to be correlated with time)\n",
    "    train_file = \"train.json\"\n",
    "    test_file = \"test.json\"\n",
    "    train_df = pd.read_json(train_file)\n",
    "    test_df = pd.read_json(test_file)\n",
    "    print(train_df.shape)\n",
    "    print(test_df.shape)\n",
    "    features_to_use  = [\"bathrooms\", \"bedrooms\", \"latitude\", \"longitude\", \"price\", \"per_sqft\",\n",
    "                       \"img_days_passed\", \"img_date_month\", \"img_date_week\", \"img_date_day\", \"img_date_dayofweek\", \"img_date_dayofyear\", \"img_date_hour\", \"img_date_monthBeginMidEnd\", \"logprice\", \"num_photos\", \"half_bathrooms\", \"num_description_words\", \"num_price_by_furniture\", \"price_latitue\", \"price_longtitude\", \"num_furniture\", \"created_month\", \"created_day\", \"created_weekday\", \"created_week\", \"created_hour\", \"total_days\", \"diff_rank\", \"density\",\n",
    "                       'num_rho', 'num_phi',\n",
    "                       'num_rot15_X', 'num_rot15_Y', 'num_rot30_X', 'num_rot30_Y',\n",
    "                       'num_rot45_X', 'num_rot45_Y', 'num_rot60_X', 'num_rot60_Y',\n",
    "                       'num_cap_share', 'num_nr_of_lines', 'num_redacted', 'num_email',\n",
    "                       'num_phone_nr']\n",
    "\n",
    "    \n",
    "    image_date = pd.read_csv(\"listing_image_time0.csv\")\n",
    "\n",
    "    # rename columns so you can join tables later on\n",
    "    image_date.columns = [\"listing_id\", \"time_stamp\"]\n",
    "\n",
    "    # reassign the only one timestamp from April, all others from Oct/Nov\n",
    "    image_date.loc[80240,\"time_stamp\"] = 1478129766 \n",
    "\n",
    "    image_date[\"img_date\"]                  = pd.to_datetime(image_date[\"time_stamp\"], unit=\"s\")\n",
    "    image_date[\"img_days_passed\"]           = (image_date[\"img_date\"].max() - image_date[\"img_date\"]).astype(\"timedelta64[D]\").astype(int)\n",
    "    image_date[\"img_date_month\"]            = image_date[\"img_date\"].dt.month\n",
    "    image_date[\"img_date_week\"]             = image_date[\"img_date\"].dt.week\n",
    "    image_date[\"img_date_day\"]              = image_date[\"img_date\"].dt.day\n",
    "    image_date[\"img_date_dayofweek\"]        = image_date[\"img_date\"].dt.dayofweek\n",
    "    image_date[\"img_date_dayofyear\"]        = image_date[\"img_date\"].dt.dayofyear\n",
    "    image_date[\"img_date_hour\"]             = image_date[\"img_date\"].dt.hour\n",
    "    image_date[\"img_date_monthBeginMidEnd\"] = image_date[\"img_date_day\"].apply(lambda x: 1 if x<10 else 2 if x<20 else 3)\n",
    "\n",
    "    train_df = pd.merge(train_df, image_date, on=\"listing_id\", how=\"left\")\n",
    "    test_df = pd.merge(test_df, image_date, on=\"listing_id\", how=\"left\")\n",
    "    \n",
    "\n",
    "    import math\n",
    "    def cart2rho(x, y):\n",
    "        rho = np.sqrt(x**2 + y**2)\n",
    "        return rho\n",
    "\n",
    "\n",
    "    def cart2phi(x, y):\n",
    "        phi = np.arctan2(y, x)\n",
    "        return phi\n",
    "\n",
    "\n",
    "    def rotation_x(row, alpha):\n",
    "        x = row['latitude']\n",
    "        y = row['longitude']\n",
    "        return x*math.cos(alpha) + y*math.sin(alpha)\n",
    "\n",
    "\n",
    "    def rotation_y(row, alpha):\n",
    "        x = row['latitude']\n",
    "        y = row['longitude']\n",
    "        return y*math.cos(alpha) - x*math.sin(alpha)\n",
    "\n",
    "\n",
    "    def add_rotation(degrees, df):\n",
    "        namex = \"rot\" + str(degrees) + \"_X\"\n",
    "        namey = \"rot\" + str(degrees) + \"_Y\"\n",
    "\n",
    "        df['num_' + namex] = df.apply(lambda row: rotation_x(row, math.pi/(180/degrees)), axis=1)\n",
    "        df['num_' + namey] = df.apply(lambda row: rotation_y(row, math.pi/(180/degrees)), axis=1)\n",
    "\n",
    "        return df\n",
    "\n",
    "    def operate_on_coordinates(tr_df, te_df):\n",
    "        for df in [tr_df, te_df]:\n",
    "            #polar coordinates system\n",
    "            df[\"num_rho\"] = df.apply(lambda x: cart2rho(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "            df[\"num_phi\"] = df.apply(lambda x: cart2phi(x[\"latitude\"] - 40.78222222, x[\"longitude\"]+73.96527777), axis=1)\n",
    "            #rotations\n",
    "            for angle in [15,30,45,60]:\n",
    "                df = add_rotation(angle, df)\n",
    "\n",
    "        return tr_df, te_df\n",
    "\n",
    "    train_df, test_df = operate_on_coordinates(train_df, test_df)\n",
    "    \n",
    "    import re\n",
    "\n",
    "    def cap_share(x):\n",
    "        return sum(1 for c in x if c.isupper())/float(len(x)+1)\n",
    "\n",
    "    for df in [train_df, test_df]:\n",
    "        # do you think that users might feel annoyed BY A DESCRIPTION THAT IS SHOUTING AT THEM?\n",
    "        df['num_cap_share'] = df['description'].apply(cap_share)\n",
    "\n",
    "        # how long in lines the desc is?\n",
    "        df['num_nr_of_lines'] = df['description'].apply(lambda x: x.count('<br /><br />'))\n",
    "\n",
    "        # is the description redacted by the website?        \n",
    "        df['num_redacted'] = 0\n",
    "        df['num_redacted'].ix[df['description'].str.contains('website_redacted')] = 1\n",
    "\n",
    "\n",
    "        # can we contact someone via e-mail to ask for the details?\n",
    "        df['num_email'] = 0\n",
    "        df['num_email'].ix[df['description'].str.contains('@')] = 1\n",
    "\n",
    "        #and... can we call them?\n",
    "\n",
    "        reg = re.compile(\".*?(\\(?\\d{3}\\D{0,3}\\d{3}\\D{0,3}\\d{4}).*?\", re.S)\n",
    "        def try_and_find_nr(description):\n",
    "            if reg.match(description) is None:\n",
    "                return 0\n",
    "            return 1\n",
    "\n",
    "        df['num_phone_nr'] = df['description'].apply(try_and_find_nr)\n",
    "    \n",
    "    \n",
    "    test_df[\"bathrooms\"].loc[19671] = 1.5\n",
    "    test_df[\"bathrooms\"].loc[22977] = 2.0\n",
    "    test_df[\"bathrooms\"].loc[63719] = 2.0\n",
    "    train_df[\"price\"] = train_df[\"price\"].clip(upper=13000)\n",
    "\n",
    "    train_df[\"logprice\"] = np.log(train_df[\"price\"])\n",
    "    test_df[\"logprice\"] = np.log(test_df[\"price\"])\n",
    "\n",
    "    train_df[\"per_sqft\"] = train_df[\"price\"]/(1+train_df[\"bedrooms\"].clip(1,4)+train_df[\"bathrooms\"].clip(0,2))\n",
    "    test_df[\"per_sqft\"] = test_df[\"price\"]/(1+test_df[\"bedrooms\"].clip(1,4)+test_df[\"bathrooms\"].clip(0,2))\n",
    "\n",
    "    # count of photos #\n",
    "    train_df[\"num_photos\"] = train_df[\"photos\"].apply(len)\n",
    "    test_df[\"num_photos\"] = test_df[\"photos\"].apply(len)\n",
    "    \n",
    "    # count of \"features\" #\n",
    "    train_df[\"num_features\"] = train_df[\"features\"].apply(len)\n",
    "    test_df[\"num_features\"] = test_df[\"features\"].apply(len)\n",
    "\n",
    "    train_df[\"listing_id\"] = train_df[\"listing_id\"] - 68119576.0\n",
    "    test_df[\"listing_id\"] =  test_df[\"listing_id\"] - 68119576.0\n",
    "    \n",
    "    \n",
    "    train_df['half_bathrooms'] = train_df[\"bathrooms\"] - train_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "    test_df['half_bathrooms'] = test_df[\"bathrooms\"] - test_df[\"bathrooms\"].apply(int)#.astype(int) # Half bathrooms? 1.5, 2.5, 3.5...\n",
    "\n",
    "    # count of words present in description column #\n",
    "    train_df[\"num_description_words\"] = train_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    "    test_df[\"num_description_words\"] = test_df[\"description\"].apply(lambda x: len(x.split(\" \")))\n",
    " \n",
    "    train_df[\"num_price_by_furniture\"] = (train_df[\"price\"])/ (train_df[\"bathrooms\"] + train_df[\"bedrooms\"] + 1.0)\n",
    "    test_df[\"num_price_by_furniture\"] =  (test_df[\"price\"])/ (test_df[\"bathrooms\"] + test_df[\"bedrooms\"] +  1.0)\n",
    "    \n",
    "    train_df[\"price_latitue\"] = (train_df[\"price\"])/ (train_df[\"latitude\"]+1.0)\n",
    "    test_df[\"price_latitue\"] =  (test_df[\"price\"])/ (test_df[\"latitude\"]+1.0)\n",
    "    \n",
    "    train_df[\"price_longtitude\"] = (train_df[\"price\"])/ (train_df[\"longitude\"]-1.0)\n",
    "    test_df[\"price_longtitude\"] =  (test_df[\"price\"])/ (test_df[\"longitude\"]-1.0)  \n",
    "\n",
    "    train_df[\"num_furniture\"] =  train_df[\"bathrooms\"] + train_df[\"bedrooms\"] \n",
    "    test_df[\"num_furniture\"] =   test_df[\"bathrooms\"] + test_df[\"bedrooms\"] \n",
    "    \n",
    "    train_df[\"num_furniture\"] = train_df[\"num_furniture\"].apply(lambda x:  str(x) if float(x)<9.5 else '10')\n",
    "    test_df[\"num_furniture\"] = test_df[\"num_furniture\"].apply(lambda x:  str(x) if float(x)<9.5 else '10')\n",
    "            \n",
    "    # convert the created column to datetime object so as to extract more features \n",
    "    train_df[\"created\"] = pd.to_datetime(train_df[\"created\"])\n",
    "    test_df[\"created\"] = pd.to_datetime(test_df[\"created\"])\n",
    "    \n",
    "    # Let us extract some features like year, month, day, hour from date columns #\n",
    "    train_df[\"created_month\"] = train_df[\"created\"].dt.month\n",
    "    test_df[\"created_month\"] = test_df[\"created\"].dt.month\n",
    "    train_df[\"created_day\"] = train_df[\"created\"].dt.day\n",
    "    test_df[\"created_day\"] = test_df[\"created\"].dt.day\n",
    "    train_df[\"created_weekday\"] = train_df[\"created\"].dt.weekday\n",
    "    test_df[\"created_weekday\"] = test_df[\"created\"].dt.weekday\n",
    "    train_df[\"created_week\"] = train_df[\"created\"].dt.week\n",
    "    test_df[\"created_week\"] = test_df[\"created\"].dt.week               \n",
    "    train_df[\"created_hour\"] = train_df[\"created\"].dt.hour\n",
    "    test_df[\"created_hour\"] = test_df[\"created\"].dt.hour\n",
    "    train_df[\"total_days\"] =   (train_df[\"created_month\"] -4.0)*30 + train_df[\"created_day\"] +  train_df[\"created_hour\"] /25.0\n",
    "    test_df[\"total_days\"] =(test_df[\"created_month\"] -4.0)*30 + test_df[\"created_day\"] +  test_df[\"created_hour\"] /25.0        \n",
    "    train_df[\"diff_rank\"]= train_df[\"total_days\"]/train_df[\"listing_id\"]\n",
    "    test_df[\"diff_rank\"]= test_df[\"total_days\"]/test_df[\"listing_id\"]\n",
    "\n",
    "    \n",
    "    train_df[\"pos\"] = train_df.longitude.round(3).astype(str) + '_' + train_df.latitude.round(3).astype(str)\n",
    "    test_df[\"pos\"] = test_df.longitude.round(3).astype(str) + '_' + test_df.latitude.round(3).astype(str)\n",
    "\n",
    "    vals = train_df['pos'].value_counts()\n",
    "    dvals = vals.to_dict()\n",
    "    train_df[\"density\"] = train_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "    test_df[\"density\"] = test_df['pos'].apply(lambda x: dvals.get(x, vals.min()))\n",
    "\n",
    "    \n",
    "    index=list(range(train_df.shape[0]))\n",
    "    random.shuffle(index)\n",
    "    a=[np.nan]*len(train_df)\n",
    "    b=[np.nan]*len(train_df)\n",
    "    c=[np.nan]*len(train_df)\n",
    "\n",
    "    for i in range(5):\n",
    "        building_level={}\n",
    "        for j in train_df['manager_id'].values:\n",
    "            building_level[j]=[0,0,0]\n",
    "\n",
    "        test_index=index[int((i*train_df.shape[0])/5):int(((i+1)*train_df.shape[0])/5)]\n",
    "        train_index=list(set(index).difference(test_index))\n",
    "\n",
    "        for j in train_index:\n",
    "            temp=train_df.iloc[j]\n",
    "            if temp['interest_level']=='low':\n",
    "                building_level[temp['manager_id']][0]+=1\n",
    "            if temp['interest_level']=='medium':\n",
    "                building_level[temp['manager_id']][1]+=1\n",
    "            if temp['interest_level']=='high':\n",
    "                building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "        for j in test_index:\n",
    "            temp=train_df.iloc[j]\n",
    "            if sum(building_level[temp['manager_id']])!=0:\n",
    "                a[j]=building_level[temp['manager_id']][0]*1.0/sum(building_level[temp['manager_id']])\n",
    "                b[j]=building_level[temp['manager_id']][1]*1.0/sum(building_level[temp['manager_id']])\n",
    "                c[j]=building_level[temp['manager_id']][2]*1.0/sum(building_level[temp['manager_id']])\n",
    "\n",
    "    train_df['manager_level_low']=a\n",
    "    train_df['manager_level_medium']=b\n",
    "    train_df['manager_level_high']=c\n",
    "\n",
    "    a=[]\n",
    "    b=[]\n",
    "    c=[]\n",
    "    building_level={}\n",
    "    for j in train_df['manager_id'].values:\n",
    "        building_level[j]=[0,0,0]\n",
    "\n",
    "    for j in range(train_df.shape[0]):\n",
    "        temp=train_df.iloc[j]\n",
    "        if temp['interest_level']=='low':\n",
    "            building_level[temp['manager_id']][0]+=1\n",
    "        if temp['interest_level']=='medium':\n",
    "            building_level[temp['manager_id']][1]+=1\n",
    "        if temp['interest_level']=='high':\n",
    "            building_level[temp['manager_id']][2]+=1\n",
    "\n",
    "    for i in test_df['manager_id'].values:\n",
    "        if i not in building_level.keys():\n",
    "            a.append(np.nan)\n",
    "            b.append(np.nan)\n",
    "            c.append(np.nan)\n",
    "        else:\n",
    "            a.append(building_level[i][0]*1.0/sum(building_level[i]))\n",
    "            b.append(building_level[i][1]*1.0/sum(building_level[i]))\n",
    "            c.append(building_level[i][2]*1.0/sum(building_level[i]))\n",
    "    test_df['manager_level_low']=a\n",
    "    test_df['manager_level_medium']=b\n",
    "    test_df['manager_level_high']=c\n",
    "\n",
    "    features_to_use.append('manager_level_low') \n",
    "    features_to_use.append('manager_level_medium') \n",
    "    features_to_use.append('manager_level_high')\n",
    "    \n",
    "    \n",
    "    categorical = [ \"display_address\", \"manager_id\", \"building_id\",\"street_address\",\"num_furniture\"]#,\"num_furniture\",\"latitude_binned\"]#\"\", \"\",\"street_address\"\n",
    "    lencat=len(categorical)\n",
    "\n",
    "    for f in range (0,lencat):\n",
    "        for s in range (f+1,lencat): \n",
    "            train_df[categorical[f] + \"_\" +categorical[s]] =train_df[categorical[f]]+\"_\" + train_df[categorical[s]]\n",
    "            test_df[categorical[f] + \"_\" +categorical[s]] =test_df[categorical[f]]+\"_\" + test_df[categorical[s]]            \n",
    "            categorical.append(categorical[f] + \"_\" +categorical[s])\n",
    "       \n",
    "    # adding all these new features to use list #\n",
    "    features_to_use.extend([\"num_photos\", \"num_features\", \"num_description_words\", \"created_month\", \"created_day\", \"listing_id\", \"created_hour\",\"total_days\",\"diff_rank\",#\"listing_rank\",\"total_days_rank\",\n",
    "    \"num_price_by_furniture\",\"price_latitue\",\"price_longtitude\"])#,\"price_latitue_longtitude\"]) \"created_year\", #,\"num_description_length\"\n",
    "    result = pd.concat([train_df,test_df])\n",
    "\n",
    "    for f in categorical:\n",
    "            if train_df[f].dtype=='object':\n",
    "\n",
    "                cases=defaultdict(int)\n",
    "                temp=np.array(result[f]).tolist()\n",
    "                for k in temp:\n",
    "                    cases[k]+=1\n",
    "                print (f, len(cases)) \n",
    "                \n",
    "                train_df[f]=train_df[f].apply(lambda x: cases[x])\n",
    "                test_df[f]=test_df[f].apply(lambda x: cases[x])               \n",
    "                \n",
    "                features_to_use.append(f)  \n",
    "\n",
    "    train_df['features'] =  train_df['features'].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))\n",
    "    test_df['features'] =test_df['features'].apply(lambda x: \" \".join([\"_\".join(i.split(\" \")) for i in x]))  \n",
    "\n",
    "    train_df['description'] =  train_df['description'].apply(lambda x: str(x).encode('utf-8') if len(x)>2 else \"nulldesc\") \n",
    "    test_df['description'] =test_df['description'].apply(lambda x: str(x).encode('utf-8') if len(x)>2 else \"nulldesc\") \n",
    "    \n",
    "    tfidfdesc=TfidfVectorizer(min_df=20, max_features=50, strip_accents='unicode',lowercase =True,\n",
    "                        analyzer='word', token_pattern=r'\\w{16,}', ngram_range=(1, 2), use_idf=False,smooth_idf=False, \n",
    "    sublinear_tf=True, stop_words = 'english')  \n",
    "    \n",
    "    print(train_df[\"features\"].head())\n",
    "       \n",
    "    tfidf = CountVectorizer(stop_words='english', max_features=200)\n",
    "    \n",
    "    te_sparse = tfidf.fit_transform(test_df[\"features\"])  \n",
    "    tr_sparse = tfidf.transform(train_df[\"features\"])   \n",
    "\n",
    "    te_sparsed = tfidfdesc. fit_transform(test_df[\"description\"])  \n",
    "    tr_sparsed = tfidfdesc.transform(train_df[\"description\"])\n",
    "    print(features_to_use)\n",
    "    \n",
    "    train_df[features_to_use] = train_df[features_to_use].fillna(train_df[features_to_use].mean())\n",
    "    test_df[features_to_use] = test_df[features_to_use].fillna(test_df[features_to_use].mean())\n",
    "\n",
    "    train_df[['manager_level_low', 'manager_level_medium', 'manager_level_high']] = train_df[['manager_level_low', 'manager_level_medium', 'manager_level_high']].fillna(0)\n",
    "    test_df[['manager_level_low', 'manager_level_medium', 'manager_level_high']] = test_df[['manager_level_low', 'manager_level_medium', 'manager_level_high']].fillna(0)    \n",
    "\n",
    "    train_X = sparse.hstack([train_df[features_to_use], tr_sparse,tr_sparsed]).tocsr()#\n",
    "    test_X = sparse.hstack([test_df[features_to_use], te_sparse,te_sparsed]).tocsr()#\n",
    "    \n",
    "    target_num_map = {'high':0, 'medium':1, 'low':2}\n",
    "    train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))\n",
    "    ids= test_df.listing_id.values\n",
    "    print(train_X.shape, test_X.shape)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#0.53436758843932697\n",
    "#With price per sq feet. 0.52887449500894068\n",
    "#Stopping. Best iteration:\n",
    "#[1477]\ttrain-mlogloss:0.35289\ttest-mlogloss:0.52884\n",
    "\n",
    "#[0.52887449500894068]\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import sparse\n",
    "import xgboost as xgb\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=321, num_rounds=4000):\n",
    "    param = {}\n",
    "    param['objective'] = 'multi:softprob'\n",
    "    param['eta'] = 0.01\n",
    "    param['max_depth'] = 6\n",
    "    param['silent'] = 1\n",
    "    param['num_class'] = 3\n",
    "    param['eval_metric'] = \"mlogloss\"\n",
    "    param['min_child_weight'] = 1\n",
    "    param['nthread'] = 16 \n",
    "    param['subsample'] = 0.7\n",
    "    param['colsample_bytree'] = 0.7\n",
    "    param['seed'] = seed_val\n",
    "    num_rounds = num_rounds\n",
    "\n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=20, verbose_eval = 100)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest)\n",
    "    return pred_test_y, model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-mlogloss:1.09146\ttest-mlogloss:1.09161\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 20 rounds.\n",
      "[100]\ttrain-mlogloss:0.714815\ttest-mlogloss:0.729486\n",
      "[200]\ttrain-mlogloss:0.594097\ttest-mlogloss:0.621037\n",
      "[300]\ttrain-mlogloss:0.541806\ttest-mlogloss:0.579213\n",
      "[400]\ttrain-mlogloss:0.511963\ttest-mlogloss:0.558776\n",
      "[500]\ttrain-mlogloss:0.491122\ttest-mlogloss:0.54722\n",
      "[600]\ttrain-mlogloss:0.474632\ttest-mlogloss:0.539553\n",
      "[700]\ttrain-mlogloss:0.460497\ttest-mlogloss:0.534197\n",
      "[800]\ttrain-mlogloss:0.448256\ttest-mlogloss:0.530062\n",
      "[900]\ttrain-mlogloss:0.437338\ttest-mlogloss:0.526799\n",
      "[1000]\ttrain-mlogloss:0.427438\ttest-mlogloss:0.524201\n",
      "[1100]\ttrain-mlogloss:0.417836\ttest-mlogloss:0.522193\n",
      "[1200]\ttrain-mlogloss:0.409025\ttest-mlogloss:0.520405\n",
      "[1300]\ttrain-mlogloss:0.40077\ttest-mlogloss:0.519107\n",
      "[1400]\ttrain-mlogloss:0.392635\ttest-mlogloss:0.518031\n",
      "[1500]\ttrain-mlogloss:0.385295\ttest-mlogloss:0.517064\n",
      "[1600]\ttrain-mlogloss:0.377914\ttest-mlogloss:0.5163\n",
      "[1700]\ttrain-mlogloss:0.370718\ttest-mlogloss:0.515721\n",
      "[1800]\ttrain-mlogloss:0.363884\ttest-mlogloss:0.515188\n",
      "[1900]\ttrain-mlogloss:0.357379\ttest-mlogloss:0.514733\n",
      "[2000]\ttrain-mlogloss:0.35084\ttest-mlogloss:0.514393\n",
      "[2100]\ttrain-mlogloss:0.34481\ttest-mlogloss:0.514013\n",
      "Stopping. Best iteration:\n",
      "[2089]\ttrain-mlogloss:0.345439\ttest-mlogloss:0.513977\n",
      "\n",
      "[0.51399072925571732]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "\n",
    "cv_scores = []\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2016)\n",
    "for dev_index, val_index in kf.split(range(train_X.shape[0])):\n",
    "        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]\n",
    "        dev_Y, val_Y = train_y[dev_index], train_y[val_index]\n",
    "        preds, model = runXGB(dev_X, dev_Y, val_X, val_Y)\n",
    "        cv_scores.append(log_loss(val_Y, preds))\n",
    "        print(cv_scores)\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2611.25"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2089*(5/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "preds, model = runXGB(train_X, train_y, test_X, num_rounds=2600)\n",
    "out_df = pd.DataFrame(preds)\n",
    "out_df.columns = [\"high\", \"medium\", \"low\"]\n",
    "#out_df[\"listing_id\"] = test_df.listing_id.values\n",
    "#out_df.to_csv(\"xgb_new.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "te = pd.read_json(\"test.json\")\n",
    "out_df[\"listing_id\"] = te.listing_id.values\n",
    "out_df.to_csv(\"xgb_new.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='xgb_new.csv' target='_blank'>xgb_new.csv</a><br>"
      ],
      "text/plain": [
       "/resources/xgb_new.csv"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('xgb_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Postprocessing of probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "interest_levels = ['low', 'medium', 'high']\n",
    "\n",
    "tau_train = {\n",
    "    'low': 0.694683, \n",
    "    'medium': 0.227529,\n",
    "    'high': 0.077788, \n",
    "}\n",
    "\n",
    "tau_test = {\n",
    "    'low': 0.69195995, \n",
    "    'medium': 0.23108864,\n",
    "    'high': 0.07695141, \n",
    "}\n",
    "\n",
    "def correct(df, train=True, verbose=False):\n",
    "    if train:\n",
    "        tau = tau_train\n",
    "    else:\n",
    "        tau = tau_test\n",
    "        \n",
    "    df_sum = df[interest_levels].sum(axis=1)\n",
    "    df_correct = df[interest_levels].copy()\n",
    "    \n",
    "    if verbose:\n",
    "        y = df_correct.mean()\n",
    "        a = [tau[k] / y[k]  for k in interest_levels]\n",
    "        print( a)\n",
    "    \n",
    "    for c in interest_levels:\n",
    "        df_correct[c] /= df_sum\n",
    "\n",
    "    for i in range(20):\n",
    "        for c in interest_levels:\n",
    "            df_correct[c] *= tau[c] / df_correct[c].mean()\n",
    "\n",
    "        df_sum = df_correct.sum(axis=1)\n",
    "\n",
    "        for c in interest_levels:\n",
    "            df_correct[c] /= df_sum\n",
    "    \n",
    "    if verbose:\n",
    "        y = df_correct.mean()\n",
    "        a = [tau[k] / y[k]  for k in interest_levels]\n",
    "        print( a)\n",
    "\n",
    "    return df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sig_xgb_nn = pd.read_csv(\"last.csv\")\n",
    "\n",
    "sig_xgb_nn_c = correct(sig_xgb_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>low</th>\n",
       "      <th>medium</th>\n",
       "      <th>high</th>\n",
       "      <th>listing_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.623037</td>\n",
       "      <td>0.289124</td>\n",
       "      <td>0.087839</td>\n",
       "      <td>7142618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.666562</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.164763</td>\n",
       "      <td>7210040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.750244</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>7103890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.585687</td>\n",
       "      <td>0.347896</td>\n",
       "      <td>0.066417</td>\n",
       "      <td>7143442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.722875</td>\n",
       "      <td>0.260359</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>6860601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        low    medium      high  listing_id\n",
       "0  0.623037  0.289124  0.087839     7142618\n",
       "1  0.666562  0.168675  0.164763     7210040\n",
       "2  0.750244  0.239467  0.010289     7103890\n",
       "3  0.585687  0.347896  0.066417     7143442\n",
       "4  0.722875  0.260359  0.016766     6860601"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_xgb_nn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sig_xgb_nn_c['listing_id'] = sig_xgb_nn['listing_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>low</th>\n",
       "      <th>medium</th>\n",
       "      <th>high</th>\n",
       "      <th>listing_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.623037</td>\n",
       "      <td>0.289124</td>\n",
       "      <td>0.087839</td>\n",
       "      <td>7142618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.666562</td>\n",
       "      <td>0.168675</td>\n",
       "      <td>0.164763</td>\n",
       "      <td>7210040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.750244</td>\n",
       "      <td>0.239467</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>7103890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.585687</td>\n",
       "      <td>0.347896</td>\n",
       "      <td>0.066417</td>\n",
       "      <td>7143442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.722875</td>\n",
       "      <td>0.260359</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>6860601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        low    medium      high  listing_id\n",
       "0  0.623037  0.289124  0.087839     7142618\n",
       "1  0.666562  0.168675  0.164763     7210040\n",
       "2  0.750244  0.239467  0.010289     7103890\n",
       "3  0.585687  0.347896  0.066417     7143442\n",
       "4  0.722875  0.260359  0.016766     6860601"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_xgb_nn_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sig_xgb_nn_c.to_csv(\"sigma_stack_pred_c.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='sigma_stack_pred_c.csv' target='_blank'>sigma_stack_pred_c.csv</a><br>"
      ],
      "text/plain": [
       "/resources/sigma_stack_pred_c.csv"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink, FileLinks\n",
    "FileLink('sigma_stack_pred_c.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
